Q-Learning
Q-Learning - Steps/Actions to Reach Terminal State - 250 Iterations
679,437,1248,303,181,176,216,148,183,306,126,159,134,135,297,172,199,68,61,169,113,73,45,112,41,195,123,82,49,36,60,113,38,63,46,37,398,49,35,72,151,61,31,32,65,49,77,28,40,42,35,100,35,77,115,60,71,41,72,52,62,45,129,53,48,46,64,32,53,31,37,46,72,53,47,45,51,58,52,41,46,32,68,44,59,53,57,38,196,71,37,108,80,92,62,89,59,43,53,61,191,47,306,63,66,46,36,39,32,35,42,94,63,45,33,32,40,39,46,40,43,67,37,45,58,46,58,57,37,70
Q-Learning - Time Required to Run Algorithm - 250 Iterations
86,32,29,23,7,10,19,10,12,18,15,10,20,11,14,33,33,18,16,13,14,20,12,13,12,19,42,34,46,15,17,18,15,16,12,18,17,18,16,19,16,19,19,48,51,54,48,44,42,17,15,17,15,17,14,15,19,15,17,16,18,17,17,17,18,18,21,20,17,21,16,25,51,53,58,56,54,44,17,20,19,22,19,19,18,27,22,21,25,23,22,23,24,27,25,32,22,23,22,22,24,23,28,23,24,69,73,75,68,60,63,87,31,30,30,25,24,24,23,31,26,24,23,26,25,29,26,29,25,24
Q-Learning - Reward Gained - 250 Iterations
-577,-335,-1146,-201,-79,-74,-114,-46,-81,-204,-24,-57,-32,-33,-195,-70,-97,34,41,-67,-11,29,57,-10,61,-93,-21,20,53,66,42,-11,64,39,56,65,-296,53,67,30,-49,41,71,70,37,53,25,74,62,60,67,2,67,25,-13,42,31,61,30,50,40,57,-27,49,54,56,38,70,49,71,65,56,30,49,55,57,51,44,50,61,56,70,34,58,43,49,45,64,-94,31,65,-6,22,10,40,13,43,59,49,41,-89,55,-204,39,36,56,66,63,70,67,60,8,39,57,69,70,62,63,56,62,59,35,65,57,44,56,44,45,65,32

Value Iteration
Value Iteration - Steps/Actions to Reach Terminal State - 80 Iterations
3188,37056,3006,835,5822,3740,4592,2037,687,201,298,81,34,31,34,34,33,36,32,30,38,28,30,30,42,26,37,30,36,34,34,29,39,32,38,30,37,31,33,31
Value Iteration - Time Required to Run Algorithm - 80 Iterations
42,2,2,2,2,3,4,4,4,5,4,5,6,6,6,6,7,8,9,8,15,22,23,21,22,22,23,23,25,26,26,28,30,13,14,14,14,14,15,15
Value Iteration - Reward Gained - 80 Iterations
-3086,-36954,-2904,-733,-5720,-3638,-4490,-1935,-585,-99,-196,21,68,71,68,68,69,66,70,72,64,74,72,72,60,76,65,72,66,68,68,73,63,70,64,72,65,71,69,71

Policy Iteration
Policy Iteration - Steps/Actions to Reach Terminal State - 80 Iterations
38489,894992,15136,6483,7460,150433,7355,31488,657,882,459,799,61773,188,38,34,32,37,33,34,35,29,29,36,34,31,29,30,32,29,37,29,28,27,32,38,29,28,34,26
Policy Iteration - Time Required to Run Algorithm - 80 Iterations
51,7,4,4,5,6,7,8,8,9,9,11,11,11,11,12,12,14,15,90,18,18,19,18,17,19,20,21,23,26,29,28,104,26,26,26,27,28,28,31
Policy Iteration - Reward Gained - 80 Iterations
-38387,-894890,-15034,-6381,-7358,-150331,-7253,-31386,-555,-780,-357,-697,-61671,-86,64,68,70,65,69,68,67,73,73,66,68,71,73,72,70,73,65,73,74,75,70,64,73,74,68,76



,AVERAGE REWARD,,AVERAGE STEPS,,iteration convergence
PI,70.07692308,,31.92307692,,14
VI,68.82142857,,33.17857143,,12
Q,-3.615384615,,-3.615384615,,7
