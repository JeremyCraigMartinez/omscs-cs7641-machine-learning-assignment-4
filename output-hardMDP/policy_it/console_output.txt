Running 5
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,3925,9999,20180,5854,5483
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,v,>,v,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,v]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,^,<,^,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,^,^,^,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,^,^,<]
[*,*,*,*,*,^,^,^,>,^,*,*,*,*,*,^,v]
[*,*,*,*,<,^,>,>,>,^,*,*,*,*,*,*,>]
[*,*,*,*,^,>,>,>,>,^,*,*,*,*,*,^,<]
[^,*,*,>,^,>,>,^,^,*,*,*,*,*,*,^,<]
[^,<,<,^,>,v,^,^,*,*,*,*,*,*,*,^,v]
[^,^,v,^,>,>,>,v,*,*,*,*,*,*,*,>,>]
[^,>,>,v,>,>,^,*,*,*,*,*,*,*,^,<,^]
[^,v,>,>,>,>,v,*,*,*,*,*,*,*,^,>,>]
[>,v,<,>,>,>,>,*,*,*,*,*,*,*,>,v,^]
[>,v,v,>,v,>,v,*,*,*,*,*,*,*,>,>,>]
[>,>,v,>,>,>,>,*,*,*,*,*,*,*,>,^,^]
[>,>,v,<,^,>,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5
Value Iteration,
Policy Iteration,3925,9999,20180,5854,5483
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5
Value Iteration,
Policy Iteration,83,18,33,46,24
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5
Value Iteration Rewards,
Policy Iteration Rewards,-3823.0,-9897.0,-20078.0,-5752.0,-5381.0
Q Learning Rewards,
Running 10
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,3925,9999,20180,5854,5483,11629,1801,8946,8375,3478,1120,764,141,237,41
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,v,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,v]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,^,<,^,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,<]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,>,^,>,^,>,^,*,*,*,*,*,>,^]
[>,*,*,>,^,^,>,^,^,*,*,*,*,*,*,>,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,>,^,>,^,^,*,*,*,*,*,*,*,^,^]
[>,^,^,>,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,>,^,>,^,*,*,*,*,*,*,*,^,>,>]
[>,^,>,>,^,^,^,*,*,*,*,*,*,*,>,v,^]
[v,v,v,^,>,^,^,*,*,*,*,*,*,*,>,>,>]
[>,>,v,>,>,^,^,*,*,*,*,*,*,*,>,^,^]
[>,>,v,<,^,>,^,*,*,*,*,*,*,*,>,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,3925,9999,20180,5854,5483,11629,1801,8946,8375,3478,1120,764,141,237,41
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,83,18,33,46,24,12,26,42,18,21,24,36,29,36,50
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10
Value Iteration Rewards,
Policy Iteration Rewards,-3823.0,-9897.0,-20078.0,-5752.0,-5381.0,-11527.0,-1699.0,-8844.0,-8273.0,-3376.0,-1018.0,-662.0,-39.0,-135.0,61.0
Q Learning Rewards,
Running 15
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,3925,9999,20180,5854,5483,11629,1801,8946,8375,3478,1120,764,141,237,41,12182,7295,265,1142,637,957,156,420,58,93,26,26,24,40,25
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,^,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,>,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,^,>,^,>,^,^,*,*,*,*,*,*,*,^,^]
[>,^,^,>,^,^,^,*,*,*,*,*,*,*,>,^,^]
[>,^,>,>,^,>,^,*,*,*,*,*,*,*,>,^,^]
[^,^,>,>,^,^,^,*,*,*,*,*,*,*,^,^,^]
[>,^,^,^,>,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,>,>,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
Value Iteration,
Policy Iteration,3925,9999,20180,5854,5483,11629,1801,8946,8375,3478,1120,764,141,237,41,12182,7295,265,1142,637,957,156,420,58,93,26,26,24,40,25
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
Value Iteration,
Policy Iteration,83,18,33,46,24,12,26,42,18,21,24,36,29,36,50,11,25,32,34,19,21,25,30,36,38,41,43,49,64,75
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
Value Iteration Rewards,
Policy Iteration Rewards,-3823.0,-9897.0,-20078.0,-5752.0,-5381.0,-11527.0,-1699.0,-8844.0,-8273.0,-3376.0,-1018.0,-662.0,-39.0,-135.0,61.0,-12080.0,-7193.0,-163.0,-1040.0,-535.0,-855.0,-54.0,-318.0,44.0,9.0,76.0,76.0,78.0,62.0,77.0
Q Learning Rewards,
Running 20
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,3925,9999,20180,5854,5483,11629,1801,8946,8375,3478,1120,764,141,237,41,12182,7295,265,1142,637,957,156,420,58,93,26,26,24,40,25,16346,40556,1261,17635,14122,435,737,364,74,41,27,24,32,34,32,25,25,24,34,27
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,^,^,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[>,^,^,>,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,>,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[>,^,^,^,>,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,>,>,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20
Value Iteration,
Policy Iteration,3925,9999,20180,5854,5483,11629,1801,8946,8375,3478,1120,764,141,237,41,12182,7295,265,1142,637,957,156,420,58,93,26,26,24,40,25,16346,40556,1261,17635,14122,435,737,364,74,41,27,24,32,34,32,25,25,24,34,27
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20
Value Iteration,
Policy Iteration,83,18,33,46,24,12,26,42,18,21,24,36,29,36,50,11,25,32,34,19,21,25,30,36,38,41,43,49,64,75,12,9,38,16,17,20,24,29,33,36,40,45,55,51,52,51,62,67,69,78
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20
Value Iteration Rewards,
Policy Iteration Rewards,-3823.0,-9897.0,-20078.0,-5752.0,-5381.0,-11527.0,-1699.0,-8844.0,-8273.0,-3376.0,-1018.0,-662.0,-39.0,-135.0,61.0,-12080.0,-7193.0,-163.0,-1040.0,-535.0,-855.0,-54.0,-318.0,44.0,9.0,76.0,76.0,78.0,62.0,77.0,-16244.0,-40454.0,-1159.0,-17533.0,-14020.0,-333.0,-635.0,-262.0,28.0,61.0,75.0,78.0,70.0,68.0,70.0,77.0,77.0,78.0,68.0,75.0
Q Learning Rewards,

Running 40
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,52900,11846,1772,5539,1712,368,243,1023,96,38,65,29,25,21,25,23,22,26,23,27,24,24,27,27,27,28,28,22,24,31,29,33,24,25,24,29,30,36,27,22
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,^,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,
Policy Iteration,52900,11846,1772,5539,1712,368,243,1023,96,38,65,29,25,21,25,23,22,26,23,27,24,24,27,27,27,28,28,22,24,31,29,33,24,25,24,29,30,36,27,22
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,
Policy Iteration,87,14,39,48,22,28,35,39,46,47,76,58,61,58,62,66,139,174,116,70,79,78,76,87,87,100,95,101,230,283,204,110,109,111,121,123,126,129,126,137
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration Rewards,
Policy Iteration Rewards,-52798.0,-11744.0,-1670.0,-5437.0,-1610.0,-266.0,-141.0,-921.0,6.0,64.0,37.0,73.0,77.0,81.0,77.0,79.0,80.0,76.0,79.0,75.0,78.0,78.0,75.0,75.0,75.0,74.0,74.0,80.0,78.0,71.0,73.0,69.0,78.0,77.0,78.0,73.0,72.0,66.0,75.0,80.0
Q Learning Rewards,

Running 80
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,4095,2174,1135,185,78,32,23,31,22,25,40,24,30,34,23,36,35,23,29,21,28,34,27,27,30,29,29,29,31,30,22,27,34,28,36,33,24,23,36,30
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80
Value Iteration,
Policy Iteration,4095,2174,1135,185,78,32,23,31,22,25,40,24,30,34,23,36,35,23,29,21,28,34,27,27,30,29,29,29,31,30,22,27,34,28,36,33,24,23,36,30
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80
Value Iteration,
Policy Iteration,104,27,47,54,101,57,69,133,114,89,95,146,279,130,122,139,138,187,321,152,148,137,140,143,331,321,168,169,179,190,188,200,240,205,209,222,225,230,246,253
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80
Value Iteration Rewards,
Policy Iteration Rewards,-3993.0,-2072.0,-1033.0,-83.0,24.0,70.0,79.0,71.0,80.0,77.0,62.0,78.0,72.0,68.0,79.0,66.0,67.0,79.0,73.0,81.0,74.0,68.0,75.0,75.0,72.0,73.0,73.0,73.0,71.0,72.0,80.0,75.0,68.0,74.0,66.0,69.0,78.0,79.0,66.0,72.0
Q Learning Rewards,
