/////////////////
// init=0.8, 0.99
/////////////////
The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,530,235,543,306,136,246,102,387,452,225,92,193,372,118,119,121,55,40,50,72,45,68,51,100,43,49,31,163,34,69,155,31,56,57,35,90,44,38,65,42,45,49,102,37,72,37,59,42,61,84,37,36,105,43,78,74,39,47,51,63,37,47,57,86,56,37,65,54,52,46,69,49,45,46,60,50,71,48,52,39,64,247,37,39,58,32,42,85,62,60,49,41,37,78,33,53,63,78,87,32

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,83,21,34,34,10,7,9,16,11,11,14,12,16,11,10,26,35,19,14,14,13,18,13,12,12,26,41,56,38,13,24,14,14,13,18,13,15,13,14,14,15,18,45,47,57,42,50,29,17,16,18,15,16,17,16,25,14,19,22,18,17,20,24,19,20,21,20,20,17,39,53,64,61,57,57,33,21,18,22,24,19,22,23,27,21,21,23,23,21,25,24,22,19,22,22,22,25,21,20,23

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-428.0,-133.0,-441.0,-204.0,-34.0,-144.0,0.0,-285.0,-350.0,-123.0,10.0,-91.0,-270.0,-16.0,-17.0,-19.0,47.0,62.0,52.0,30.0,57.0,34.0,51.0,2.0,59.0,53.0,71.0,-61.0,68.0,33.0,-53.0,71.0,46.0,45.0,67.0,12.0,58.0,64.0,37.0,60.0,57.0,53.0,0.0,65.0,30.0,65.0,43.0,60.0,41.0,18.0,65.0,66.0,-3.0,59.0,24.0,28.0,63.0,55.0,51.0,39.0,65.0,55.0,45.0,16.0,46.0,65.0,37.0,48.0,50.0,56.0,33.0,53.0,57.0,56.0,42.0,52.0,31.0,54.0,50.0,63.0,38.0,-145.0,65.0,63.0,44.0,70.0,60.0,17.0,40.0,42.0,53.0,61.0,65.0,24.0,69.0,49.0,39.0,24.0,15.0,70.0

/////////////////
// 0.2, 0.99
/////////////////
The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,455,470,118,176,203,116,350,173,474,193,111,204,61,47,94,75,271,38,145,74,63,224,81,197,277,82,40,49,40,48,99,41,51,53,41,53,49,31,42,56,48,38,112,39,37,114,37,50,48,52,107,65,36,68,73,53,44,66,36,51,85,37,83,59,40,112,67,33,50,74,49,47,49,40,41,33,65,110,53,34,56,30,46,47,30,41,48,82,40,45,76,31,40,56,87,78,50,56,50,45

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,79,25,29,31,11,10,12,17,13,14,12,12,19,12,11,30,26,12,14,11,16,14,13,13,20,31,38,33,31,13,12,14,15,17,23,20,16,12,13,15,17,13,26,40,52,44,41,50,40,15,21,17,19,19,18,17,16,17,16,17,24,18,18,19,18,16,18,21,21,17,30,57,52,51,77,79,30,25,23,19,20,21,28,21,22,22,21,25,22,26,28,25,23,24,26,26,24,22,23,24

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-353.0,-368.0,-16.0,-74.0,-101.0,-14.0,-248.0,-71.0,-372.0,-91.0,-9.0,-102.0,41.0,55.0,8.0,27.0,-169.0,64.0,-43.0,28.0,39.0,-122.0,21.0,-95.0,-175.0,20.0,62.0,53.0,62.0,54.0,3.0,61.0,51.0,49.0,61.0,49.0,53.0,71.0,60.0,46.0,54.0,64.0,-10.0,63.0,65.0,-12.0,65.0,52.0,54.0,50.0,-5.0,37.0,66.0,34.0,29.0,49.0,58.0,36.0,66.0,51.0,17.0,65.0,19.0,43.0,62.0,-10.0,35.0,69.0,52.0,28.0,53.0,55.0,53.0,62.0,61.0,69.0,37.0,-8.0,49.0,68.0,46.0,72.0,56.0,55.0,72.0,61.0,54.0,20.0,62.0,57.0,26.0,71.0,62.0,46.0,15.0,24.0,52.0,46.0,52.0,57.0

/////////////////
// 0.99, L=0.8
/////////////////
The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,283,134,399,479,135,123,130,150,116,78,88,110,181,82,61,178,105,40,119,54,62,243,72,43,34,85,51,32,68,407,29,51,32,40,52,33,33,45,40,46,35,45,35,37,40,72,41,47,52,46,31,45,40,55,32,30,68,51,34,38,41,33,41,48,35,42,48,38,36,36,40,33,30,31,53,71,43,42,51,41,44,40,46,35,37,44,72,45,40,54,47,36,32,49,41,39,41,54,54,37

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,84,19,25,26,22,8,24,14,12,17,16,11,18,11,11,15,42,23,11,13,13,19,14,14,12,14,36,33,40,28,12,13,14,14,15,14,14,15,14,13,13,14,16,21,39,47,49,51,49,42,18,18,17,19,17,16,20,15,17,18,18,22,15,16,17,17,16,17,20,20,16,17,16,38,62,52,57,57,64,35,20,23,20,22,18,20,23,20,20,19,20,23,21,19,24,23,21,21,20,19

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-181.0,-32.0,-297.0,-377.0,-33.0,-21.0,-28.0,-48.0,-14.0,24.0,14.0,-8.0,-79.0,20.0,41.0,-76.0,-3.0,62.0,-17.0,48.0,40.0,-141.0,30.0,59.0,68.0,17.0,51.0,70.0,34.0,-305.0,73.0,51.0,70.0,62.0,50.0,69.0,69.0,57.0,62.0,56.0,67.0,57.0,67.0,65.0,62.0,30.0,61.0,55.0,50.0,56.0,71.0,57.0,62.0,47.0,70.0,72.0,34.0,51.0,68.0,64.0,61.0,69.0,61.0,54.0,67.0,60.0,54.0,64.0,66.0,66.0,62.0,69.0,72.0,71.0,49.0,31.0,59.0,60.0,51.0,61.0,58.0,62.0,56.0,67.0,65.0,58.0,30.0,57.0,62.0,48.0,55.0,66.0,70.0,53.0,61.0,63.0,61.0,48.0,48.0,65.0

/////////////////
// 0.99, L=0.2
/////////////////
The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,1932,357,545,216,655,263,463,999,1129,318,796,156,271,239,190,110,219,123,257,285,204,163,265,121,217,94,113,123,189,191,344,105,98,75,102,112,127,100,66,221,58,42,97,62,84,48,70,49,271,110,83,63,151,89,44,106,68,126,36,69,43,45,35,37,47,41,46,116,51,33,140,39,32,56,36,71,40,36,51,35,36,46,53,35,40,37,32,37,32,32,35,37,52,35,37,209,39,35,37,44

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,105,23,36,20,22,14,11,20,27,20,22,57,18,17,18,24,18,17,37,64,42,26,23,28,24,25,24,25,22,57,65,76,74,37,29,26,27,26,30,26,27,29,29,29,28,33,32,47,96,86,92,67,32,30,28,29,34,32,30,34,29,35,39,34,35,33,37,35,35,35,34,55,92,98,92,100,91,66,37,35,36,36,37,34,38,35,40,38,42,43,43,43,39,37,36,39,40,38,40,46

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-1830.0,-255.0,-443.0,-114.0,-553.0,-161.0,-361.0,-897.0,-1027.0,-216.0,-694.0,-54.0,-169.0,-137.0,-88.0,-8.0,-117.0,-21.0,-155.0,-183.0,-102.0,-61.0,-163.0,-19.0,-115.0,8.0,-11.0,-21.0,-87.0,-89.0,-242.0,-3.0,4.0,27.0,0.0,-10.0,-25.0,2.0,36.0,-119.0,44.0,60.0,5.0,40.0,18.0,54.0,32.0,53.0,-169.0,-8.0,19.0,39.0,-49.0,13.0,58.0,-4.0,34.0,-24.0,66.0,33.0,59.0,57.0,67.0,65.0,55.0,61.0,56.0,-14.0,51.0,69.0,-38.0,63.0,70.0,46.0,66.0,31.0,62.0,66.0,51.0,67.0,66.0,56.0,49.0,67.0,62.0,65.0,70.0,65.0,70.0,70.0,67.0,65.0,50.0,67.0,65.0,-107.0,63.0,67.0,65.0,58.0

/////////////////
// control
/////////////////
The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,288,1609,138,189,190,407,167,57,85,104,62,131,136,52,155,345,47,193,40,46,85,47,67,44,66,70,92,138,149,64,33,158,51,43,64,54,47,53,146,54,73,58,55,35,46,35,83,85,49,207,44,57,77,36,81,47,51,67,32,36,76,60,49,47,48,41,40,97,53,39,48,43,44,67,118,42,56,52,53,49,43,36,40,38,51,32,49,65,44,36,181,56,183,47,58,61,45,85,41,67

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,87,54,26,24,11,10,9,16,11,15,9,11,17,11,14,33,36,13,12,13,20,12,13,12,16,33,38,39,36,18,15,24,21,17,18,21,20,15,22,18,17,25,45,50,51,52,50,46,16,23,19,19,22,19,20,17,18,19,27,16,18,16,18,17,17,18,17,17,18,33,56,53,51,51,64,43,21,18,19,22,24,25,20,20,20,22,23,24,28,27,22,22,23,21,25,23,21,28,26,26

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-186.0,-1507.0,-36.0,-87.0,-88.0,-305.0,-65.0,45.0,17.0,-2.0,40.0,-29.0,-34.0,50.0,-53.0,-243.0,55.0,-91.0,62.0,56.0,17.0,55.0,35.0,58.0,36.0,32.0,10.0,-36.0,-47.0,38.0,69.0,-56.0,51.0,59.0,38.0,48.0,55.0,49.0,-44.0,48.0,29.0,44.0,47.0,67.0,56.0,67.0,19.0,17.0,53.0,-105.0,58.0,45.0,25.0,66.0,21.0,55.0,51.0,35.0,70.0,66.0,26.0,42.0,53.0,55.0,54.0,61.0,62.0,5.0,49.0,63.0,54.0,59.0,58.0,35.0,-16.0,60.0,46.0,50.0,49.0,53.0,59.0,66.0,62.0,64.0,51.0,70.0,53.0,37.0,58.0,66.0,-79.0,46.0,-81.0,55.0,44.0,41.0,57.0,17.0,61.0,35.0


actions
530,235,543,306,136,246,102,387,452,225,92,193,372,118,119,121,55,40,50,72,45,68,51,100,43,49,31,163,34,69,155,31,56,57,35,90,44,38,65,42,45,49,102,37,72,37,59,42,61,84,37,36,105,43,78,74,39,47,51,63,37,47,57,86,56,37,65,54,52,46,69,49,45,46,60,50,71,48,52,39,64,247,37,39,58,32,42,85,62,60,49,41,37,78,33,53,63,78,87,32
455,470,118,176,203,116,350,173,474,193,111,204,61,47,94,75,271,38,145,74,63,224,81,197,277,82,40,49,40,48,99,41,51,53,41,53,49,31,42,56,48,38,112,39,37,114,37,50,48,52,107,65,36,68,73,53,44,66,36,51,85,37,83,59,40,112,67,33,50,74,49,47,49,40,41,33,65,110,53,34,56,30,46,47,30,41,48,82,40,45,76,31,40,56,87,78,50,56,50,45
283,134,399,479,135,123,130,150,116,78,88,110,181,82,61,178,105,40,119,54,62,243,72,43,34,85,51,32,68,407,29,51,32,40,52,33,33,45,40,46,35,45,35,37,40,72,41,47,52,46,31,45,40,55,32,30,68,51,34,38,41,33,41,48,35,42,48,38,36,36,40,33,30,31,53,71,43,42,51,41,44,40,46,35,37,44,72,45,40,54,47,36,32,49,41,39,41,54,54,37
1932,357,545,216,655,263,463,999,1129,318,796,156,271,239,190,110,219,123,257,285,204,163,265,121,217,94,113,123,189,191,344,105,98,75,102,112,127,100,66,221,58,42,97,62,84,48,70,49,271,110,83,63,151,89,44,106,68,126,36,69,43,45,35,37,47,41,46,116,51,33,140,39,32,56,36,71,40,36,51,35,36,46,53,35,40,37,32,37,32,32,35,37,52,35,37,209,39,35,37,44
288,1609,138,189,190,407,167,57,85,104,62,131,136,52,155,345,47,193,40,46,85,47,67,44,66,70,92,138,149,64,33,158,51,43,64,54,47,53,146,54,73,58,55,35,46,35,83,85,49,207,44,57,77,36,81,47,51,67,32,36,76,60,49,47,48,41,40,97,53,39,48,43,44,67,118,42,56,52,53,49,43,36,40,38,51,32,49,65,44,36,181,56,183,47,58,61,45,85,41,67

time
83,21,34,34,10,7,9,16,11,11,14,12,16,11,10,26,35,19,14,14,13,18,13,12,12,26,41,56,38,13,24,14,14,13,18,13,15,13,14,14,15,18,45,47,57,42,50,29,17,16,18,15,16,17,16,25,14,19,22,18,17,20,24,19,20,21,20,20,17,39,53,64,61,57,57,33,21,18,22,24,19,22,23,27,21,21,23,23,21,25,24,22,19,22,22,22,25,21,20,23
79,25,29,31,11,10,12,17,13,14,12,12,19,12,11,30,26,12,14,11,16,14,13,13,20,31,38,33,31,13,12,14,15,17,23,20,16,12,13,15,17,13,26,40,52,44,41,50,40,15,21,17,19,19,18,17,16,17,16,17,24,18,18,19,18,16,18,21,21,17,30,57,52,51,77,79,30,25,23,19,20,21,28,21,22,22,21,25,22,26,28,25,23,24,26,26,24,22,23,24
84,19,25,26,22,8,24,14,12,17,16,11,18,11,11,15,42,23,11,13,13,19,14,14,12,14,36,33,40,28,12,13,14,14,15,14,14,15,14,13,13,14,16,21,39,47,49,51,49,42,18,18,17,19,17,16,20,15,17,18,18,22,15,16,17,17,16,17,20,20,16,17,16,38,62,52,57,57,64,35,20,23,20,22,18,20,23,20,20,19,20,23,21,19,24,23,21,21,20,19
105,23,36,20,22,14,11,20,27,20,22,57,18,17,18,24,18,17,37,64,42,26,23,28,24,25,24,25,22,57,65,76,74,37,29,26,27,26,30,26,27,29,29,29,28,33,32,47,96,86,92,67,32,30,28,29,34,32,30,34,29,35,39,34,35,33,37,35,35,35,34,55,92,98,92,100,91,66,37,35,36,36,37,34,38,35,40,38,42,43,43,43,39,37,36,39,40,38,40,46
87,54,26,24,11,10,9,16,11,15,9,11,17,11,14,33,36,13,12,13,20,12,13,12,16,33,38,39,36,18,15,24,21,17,18,21,20,15,22,18,17,25,45,50,51,52,50,46,16,23,19,19,22,19,20,17,18,19,27,16,18,16,18,17,17,18,17,17,18,33,56,53,51,51,64,43,21,18,19,22,24,25,20,20,20,22,23,24,28,27,22,22,23,21,25,23,21,28,26,26

reward
-428.0,-133.0,-441.0,-204.0,-34.0,-144.0,0.0,-285.0,-350.0,-123.0,10.0,-91.0,-270.0,-16.0,-17.0,-19.0,47.0,62.0,52.0,30.0,57.0,34.0,51.0,2.0,59.0,53.0,71.0,-61.0,68.0,33.0,-53.0,71.0,46.0,45.0,67.0,12.0,58.0,64.0,37.0,60.0,57.0,53.0,0.0,65.0,30.0,65.0,43.0,60.0,41.0,18.0,65.0,66.0,-3.0,59.0,24.0,28.0,63.0,55.0,51.0,39.0,65.0,55.0,45.0,16.0,46.0,65.0,37.0,48.0,50.0,56.0,33.0,53.0,57.0,56.0,42.0,52.0,31.0,54.0,50.0,63.0,38.0,-145.0,65.0,63.0,44.0,70.0,60.0,17.0,40.0,42.0,53.0,61.0,65.0,24.0,69.0,49.0,39.0,24.0,15.0,70.0
-353.0,-368.0,-16.0,-74.0,-101.0,-14.0,-248.0,-71.0,-372.0,-91.0,-9.0,-102.0,41.0,55.0,8.0,27.0,-169.0,64.0,-43.0,28.0,39.0,-122.0,21.0,-95.0,-175.0,20.0,62.0,53.0,62.0,54.0,3.0,61.0,51.0,49.0,61.0,49.0,53.0,71.0,60.0,46.0,54.0,64.0,-10.0,63.0,65.0,-12.0,65.0,52.0,54.0,50.0,-5.0,37.0,66.0,34.0,29.0,49.0,58.0,36.0,66.0,51.0,17.0,65.0,19.0,43.0,62.0,-10.0,35.0,69.0,52.0,28.0,53.0,55.0,53.0,62.0,61.0,69.0,37.0,-8.0,49.0,68.0,46.0,72.0,56.0,55.0,72.0,61.0,54.0,20.0,62.0,57.0,26.0,71.0,62.0,46.0,15.0,24.0,52.0,46.0,52.0,57.0
-181.0,-32.0,-297.0,-377.0,-33.0,-21.0,-28.0,-48.0,-14.0,24.0,14.0,-8.0,-79.0,20.0,41.0,-76.0,-3.0,62.0,-17.0,48.0,40.0,-141.0,30.0,59.0,68.0,17.0,51.0,70.0,34.0,-305.0,73.0,51.0,70.0,62.0,50.0,69.0,69.0,57.0,62.0,56.0,67.0,57.0,67.0,65.0,62.0,30.0,61.0,55.0,50.0,56.0,71.0,57.0,62.0,47.0,70.0,72.0,34.0,51.0,68.0,64.0,61.0,69.0,61.0,54.0,67.0,60.0,54.0,64.0,66.0,66.0,62.0,69.0,72.0,71.0,49.0,31.0,59.0,60.0,51.0,61.0,58.0,62.0,56.0,67.0,65.0,58.0,30.0,57.0,62.0,48.0,55.0,66.0,70.0,53.0,61.0,63.0,61.0,48.0,48.0,65.0
-1830.0,-255.0,-443.0,-114.0,-553.0,-161.0,-361.0,-897.0,-1027.0,-216.0,-694.0,-54.0,-169.0,-137.0,-88.0,-8.0,-117.0,-21.0,-155.0,-183.0,-102.0,-61.0,-163.0,-19.0,-115.0,8.0,-11.0,-21.0,-87.0,-89.0,-242.0,-3.0,4.0,27.0,0.0,-10.0,-25.0,2.0,36.0,-119.0,44.0,60.0,5.0,40.0,18.0,54.0,32.0,53.0,-169.0,-8.0,19.0,39.0,-49.0,13.0,58.0,-4.0,34.0,-24.0,66.0,33.0,59.0,57.0,67.0,65.0,55.0,61.0,56.0,-14.0,51.0,69.0,-38.0,63.0,70.0,46.0,66.0,31.0,62.0,66.0,51.0,67.0,66.0,56.0,49.0,67.0,62.0,65.0,70.0,65.0,70.0,70.0,67.0,65.0,50.0,67.0,65.0,-107.0,63.0,67.0,65.0,58.0
-186.0,-1507.0,-36.0,-87.0,-88.0,-305.0,-65.0,45.0,17.0,-2.0,40.0,-29.0,-34.0,50.0,-53.0,-243.0,55.0,-91.0,62.0,56.0,17.0,55.0,35.0,58.0,36.0,32.0,10.0,-36.0,-47.0,38.0,69.0,-56.0,51.0,59.0,38.0,48.0,55.0,49.0,-44.0,48.0,29.0,44.0,47.0,67.0,56.0,67.0,19.0,17.0,53.0,-105.0,58.0,45.0,25.0,66.0,21.0,55.0,51.0,35.0,70.0,66.0,26.0,42.0,53.0,55.0,54.0,61.0,62.0,5.0,49.0,63.0,54.0,59.0,58.0,35.0,-16.0,60.0,46.0,50.0,49.0,53.0,59.0,66.0,62.0,64.0,51.0,70.0,53.0,37.0,58.0,66.0,-79.0,46.0,-81.0,55.0,44.0,41.0,57.0,17.0,61.0,35.0
