Running 5
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,130211,4566,8108,1925,1994

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,v,>,v,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,v,v,v,v,*,*,v,v,<,v]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,<,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,^,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,^,^,^,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,^,^,<]
[*,*,*,*,*,>,>,^,>,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,>,^,>,^,>,^,*,*,*,*,*,^,<]
[^,*,*,>,>,^,^,^,^,*,*,*,*,*,*,^,<]
[^,<,<,>,^,^,^,^,*,*,*,*,*,*,*,^,v]
[^,<,^,>,^,>,^,<,*,*,*,*,*,*,*,>,>]
[^,<,>,v,^,^,^,*,*,*,*,*,*,*,<,<,^]
[v,v,>,^,<,>,^,*,*,*,*,*,*,*,^,>,>]
[>,<,<,^,>,>,^,*,*,*,*,*,*,*,>,v,^]
[^,^,<,<,v,>,v,*,*,*,*,*,*,*,v,v,>]
[^,^,v,>,v,>,>,*,*,*,*,*,*,*,>,v,<]
[>,>,v,<,v,>,^,*,*,*,*,*,*,*,>,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5
Value Iteration,130211,4566,8108,1925,1994
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5
Value Iteration,66,14,17,9,10
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5
Value Iteration Rewards,-130109.0,-4464.0,-8006.0,-1823.0,-1892.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 10
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,v]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,^,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,^,<,^,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,^,<]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,>,^,>,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,>,^,^,^,^,*,*,*,*,*,*,>,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,>,^,>,^,^,*,*,*,*,*,*,*,^,<]
[>,^,^,>,^,^,^,*,*,*,*,*,*,*,>,^,^]
[>,^,^,>,^,>,^,*,*,*,*,*,*,*,>,^,^]
[^,^,>,>,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,>,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,>,^,^,*,*,*,*,*,*,*,>,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10
Value Iteration,66,14,17,9,10,7,5,7,10,10,11,13,17,18,22
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10
Value Iteration Rewards,-130109.0,-4464.0,-8006.0,-1823.0,-1892.0,-110250.0,-18504.0,-5131.0,-479.0,-1513.0,-598.0,43.0,26.0,-307.0,75.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 15
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27,36673,9945,4693,2935,4567,1167,216,117,49,29,26,37,29,30,30

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,<]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,^,^,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[>,^,^,>,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,>,>,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,>,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,>,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27,36673,9945,4693,2935,4567,1167,216,117,49,29,26,37,29,30,30
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
Value Iteration,66,14,17,9,10,7,5,7,10,10,11,13,17,18,22,7,5,7,8,10,12,12,14,19,18,21,34,30,28,31
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
Value Iteration Rewards,-130109.0,-4464.0,-8006.0,-1823.0,-1892.0,-110250.0,-18504.0,-5131.0,-479.0,-1513.0,-598.0,43.0,26.0,-307.0,75.0,-36571.0,-9843.0,-4591.0,-2833.0,-4465.0,-1065.0,-114.0,-15.0,53.0,73.0,76.0,65.0,73.0,72.0,72.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 20
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27,36673,9945,4693,2935,4567,1167,216,117,49,29,26,37,29,30,30,3074,1859,8991,10665,3063,144,540,95,582,32,29,30,27,34,24,27,30,35,29,26

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,>,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,>,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,>,>,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27,36673,9945,4693,2935,4567,1167,216,117,49,29,26,37,29,30,30,3074,1859,8991,10665,3063,144,540,95,582,32,29,30,27,34,24,27,30,35,29,26
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20
Value Iteration,66,14,17,9,10,7,5,7,10,10,11,13,17,18,22,7,5,7,8,10,12,12,14,19,18,21,34,30,28,31,8,5,8,8,12,11,13,14,15,18,19,21,24,27,29,30,35,37,31,38
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20
Value Iteration Rewards,-130109.0,-4464.0,-8006.0,-1823.0,-1892.0,-110250.0,-18504.0,-5131.0,-479.0,-1513.0,-598.0,43.0,26.0,-307.0,75.0,-36571.0,-9843.0,-4591.0,-2833.0,-4465.0,-1065.0,-114.0,-15.0,53.0,73.0,76.0,65.0,73.0,72.0,72.0,-2972.0,-1757.0,-8889.0,-10563.0,-2961.0,-42.0,-438.0,7.0,-480.0,70.0,73.0,72.0,75.0,68.0,78.0,75.0,72.0,67.0,73.0,76.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 40
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27,36673,9945,4693,2935,4567,1167,216,117,49,29,26,37,29,30,30,3074,1859,8991,10665,3063,144,540,95,582,32,29,30,27,34,24,27,30,35,29,26,49174,25066,655,913,365,596,859,284,154,30,26,32,27,30,23,25,29,34,36,33,29,27,32,33,38,26,25,26,23,39,38,26,31,33,25,24,35,32,27,31

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,<,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27,36673,9945,4693,2935,4567,1167,216,117,49,29,26,37,29,30,30,3074,1859,8991,10665,3063,144,540,95,582,32,29,30,27,34,24,27,30,35,29,26,49174,25066,655,913,365,596,859,284,154,30,26,32,27,30,23,25,29,34,36,33,29,27,32,33,38,26,25,26,23,39,38,26,31,33,25,24,35,32,27,31
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,66,14,17,9,10,7,5,7,10,10,11,13,17,18,22,7,5,7,8,10,12,12,14,19,18,21,34,30,28,31,8,5,8,8,12,11,13,14,15,18,19,21,24,27,29,30,35,37,31,38,6,5,6,10,9,11,14,14,17,21,20,21,22,26,28,32,31,35,33,34,38,39,50,39,43,45,44,47,50,55,49,55,62,53,58,56,64,68,69,76
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration Rewards,-130109.0,-4464.0,-8006.0,-1823.0,-1892.0,-110250.0,-18504.0,-5131.0,-479.0,-1513.0,-598.0,43.0,26.0,-307.0,75.0,-36571.0,-9843.0,-4591.0,-2833.0,-4465.0,-1065.0,-114.0,-15.0,53.0,73.0,76.0,65.0,73.0,72.0,72.0,-2972.0,-1757.0,-8889.0,-10563.0,-2961.0,-42.0,-438.0,7.0,-480.0,70.0,73.0,72.0,75.0,68.0,78.0,75.0,72.0,67.0,73.0,76.0,-49072.0,-24964.0,-553.0,-811.0,-263.0,-494.0,-757.0,-182.0,-52.0,72.0,76.0,70.0,75.0,72.0,79.0,77.0,73.0,68.0,66.0,69.0,73.0,75.0,70.0,69.0,64.0,76.0,77.0,76.0,79.0,63.0,64.0,76.0,71.0,69.0,77.0,78.0,67.0,70.0,75.0,71.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 80
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,130211,4566,8108,1925,1994,110352,18606,5233,581,1615,700,59,76,409,27,36673,9945,4693,2935,4567,1167,216,117,49,29,26,37,29,30,30,3074,1859,8991,10665,3063,144,540,95,582,32,29,30,27,34,24,27,30,35,29,26,49174,25066,655,913,365,596,859,284,154,30,26,32,27,30,23,25,29,34,36,33,29,27,32,33,38,26,25,26,23,39,38,26,31,33,25,24,35,32,27,31,4617,810,490,361,26,32,22,23,30,26,28,25,27,25,23,25,24,29,29,23,31,30,27,26,25,32,33,25,33,26,29,30,36,29,28,32,39,32,29,25

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80
Value Iteration,55472,15816,5692,9310,2228,622,241,1246,117,30,39,28,23,32,35,30,25,25,30,32,26,32,26,29,27,26,28,37,32,32,32,29,32,23,22,25,27,31,26,30,35,23,24,26,31,30,29,30,26,43,26,23,32,27,26,27,28,32,30,27,26,23,30,31,24,29,30,22,29,26,25,30,29,22,25,26,28,28,27,21
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80
Value Iteration,72,7,17,13,13,12,15,17,21,21,34,26,28,30,32,36,35,35,33,43,40,43,104,109,115,94,52,52,57,61,60,57,63,63,77,74,71,64,74,82,82,92,157,189,195,192,86,87,91,91,94,97,86,93,95,102,99,95,102,103,116,123,125,121,153,149,133,119,129,137,119,126,126,127,132,135,136,137,138,137
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80
Value Iteration Rewards,-55370.0,-15714.0,-5590.0,-9208.0,-2126.0,-520.0,-139.0,-1144.0,-15.0,72.0,63.0,74.0,79.0,70.0,67.0,72.0,77.0,77.0,72.0,70.0,76.0,70.0,76.0,73.0,75.0,76.0,74.0,65.0,70.0,70.0,70.0,73.0,70.0,79.0,80.0,77.0,75.0,71.0,76.0,72.0,67.0,79.0,78.0,76.0,71.0,72.0,73.0,72.0,76.0,59.0,76.0,79.0,70.0,75.0,76.0,75.0,74.0,70.0,72.0,75.0,76.0,79.0,72.0,71.0,78.0,73.0,72.0,80.0,73.0,76.0,77.0,72.0,73.0,80.0,77.0,76.0,74.0,74.0,75.0,81.0
Policy Iteration Rewards,
Q Learning Rewards,
