Num generated: 600; num unique: 50
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130
Value Iteration,
Policy Iteration,
Q Learning,679,437,1248,303,181,176,216,148,183,306,126,159,134,135,297,172,199,68,61,169,113,73,45,112,41,195,123,82,49,36,60,113,38,63,46,37,398,49,35,72,151,61,31,32,65,49,77,28,40,42,35,100,35,77,115,60,71,41,72,52,62,45,129,53,48,46,64,32,53,31,37,46,72,53,47,45,51,58,52,41,46,32,68,44,59,53,57,38,196,71,37,108,80,92,62,89,59,43,53,61,191,47,306,63,66,46,36,39,32,35,42,94,63,45,33,32,40,39,46,40,43,67,37,45,58,46,58,57,37,70

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130
Value Iteration,
Policy Iteration,
Q Learning,86,32,29,23,7,10,19,10,12,18,15,10,20,11,14,33,33,18,16,13,14,20,12,13,12,19,42,34,46,15,17,18,15,16,12,18,17,18,16,19,16,19,19,48,51,54,48,44,42,17,15,17,15,17,14,15,19,15,17,16,18,17,17,17,18,18,21,20,17,21,16,25,51,53,58,56,54,44,17,20,19,22,19,19,18,27,22,21,25,23,22,23,24,27,25,32,22,23,22,22,24,23,28,23,24,69,73,75,68,60,63,87,31,30,30,25,24,24,23,31,26,24,23,26,25,29,26,29,25,24

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-577.0,-335.0,-1146.0,-201.0,-79.0,-74.0,-114.0,-46.0,-81.0,-204.0,-24.0,-57.0,-32.0,-33.0,-195.0,-70.0,-97.0,34.0,41.0,-67.0,-11.0,29.0,57.0,-10.0,61.0,-93.0,-21.0,20.0,53.0,66.0,42.0,-11.0,64.0,39.0,56.0,65.0,-296.0,53.0,67.0,30.0,-49.0,41.0,71.0,70.0,37.0,53.0,25.0,74.0,62.0,60.0,67.0,2.0,67.0,25.0,-13.0,42.0,31.0,61.0,30.0,50.0,40.0,57.0,-27.0,49.0,54.0,56.0,38.0,70.0,49.0,71.0,65.0,56.0,30.0,49.0,55.0,57.0,51.0,44.0,50.0,61.0,56.0,70.0,34.0,58.0,43.0,49.0,45.0,64.0,-94.0,31.0,65.0,-6.0,22.0,10.0,40.0,13.0,43.0,59.0,49.0,41.0,-89.0,55.0,-204.0,39.0,36.0,56.0,66.0,63.0,70.0,67.0,60.0,8.0,39.0,57.0,69.0,70.0,62.0,63.0,56.0,62.0,59.0,35.0,65.0,57.0,44.0,56.0,44.0,45.0,65.0,32.0
