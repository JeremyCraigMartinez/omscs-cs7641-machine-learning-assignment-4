Running 10
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,v]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,<,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,^,<,^,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,^,<]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,>,^,>,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,>,^,^,^,^,*,*,*,*,*,*,>,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,>,^,>,^,^,*,*,*,*,*,*,*,^,<]
[>,^,^,>,^,^,^,*,*,*,*,*,*,*,>,^,^]
[>,^,^,>,^,>,^,*,*,*,*,*,*,*,>,^,^]
[^,^,>,>,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,>,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,>,^,>,^,^,*,*,*,*,*,*,*,>,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,79,7,9,19,14,13,16,17,22,25
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration Rewards,-106421.0,-9166.0,-23570.0,-10739.0,-1683.0,-215.0,-1271.0,-870.0,45.0,71.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 30
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,79,7,9,19,14,13,16,17,22,25,8,5,6,10,12,14,14,31,32,50,57,63,66,72,85,86,87,95,42,52,53,59,57,57,59,56,46,49,54,53
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration Rewards,-106421.0,-9166.0,-23570.0,-10739.0,-1683.0,-215.0,-1271.0,-870.0,45.0,71.0,-28524.0,-5804.0,-6699.0,-9457.0,-1178.0,-437.0,-227.0,-385.0,-182.0,75.0,65.0,77.0,79.0,79.0,74.0,69.0,70.0,73.0,79.0,67.0,65.0,80.0,66.0,70.0,72.0,71.0,63.0,73.0,70.0,76.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 50
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26,72345,3806,18153,2914,3486,1643,618,1491,192,26,36,24,30,28,24,34,27,25,27,32,33,28,33,25,29,46,34,33,29,26,26,24,31,27,27,37,28,26,30,26,27,25,25,29,24,36,25,37,25,34

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,^,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26,72345,3806,18153,2914,3486,1643,618,1491,192,26,36,24,30,28,24,34,27,25,27,32,33,28,33,25,29,46,34,33,29,26,26,24,31,27,27,37,28,26,30,26,27,25,25,29,24,36,25,37,25,34
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration,79,7,9,19,14,13,16,17,22,25,8,5,6,10,12,14,14,31,32,50,57,63,66,72,85,86,87,95,42,52,53,59,57,57,59,56,46,49,54,53,6,6,8,10,13,17,22,23,25,24,30,25,28,29,36,34,33,32,45,46,41,36,37,40,43,45,50,68,77,77,79,84,83,77,71,77,62,67,64,64,73,95,100,98,95,81,82,91,86,84
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration Rewards,-106421.0,-9166.0,-23570.0,-10739.0,-1683.0,-215.0,-1271.0,-870.0,45.0,71.0,-28524.0,-5804.0,-6699.0,-9457.0,-1178.0,-437.0,-227.0,-385.0,-182.0,75.0,65.0,77.0,79.0,79.0,74.0,69.0,70.0,73.0,79.0,67.0,65.0,80.0,66.0,70.0,72.0,71.0,63.0,73.0,70.0,76.0,-72243.0,-3704.0,-18051.0,-2812.0,-3384.0,-1541.0,-516.0,-1389.0,-90.0,76.0,66.0,78.0,72.0,74.0,78.0,68.0,75.0,77.0,75.0,70.0,69.0,74.0,69.0,77.0,73.0,56.0,68.0,69.0,73.0,76.0,76.0,78.0,71.0,75.0,75.0,65.0,74.0,76.0,72.0,76.0,75.0,77.0,77.0,73.0,78.0,66.0,77.0,65.0,77.0,68.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 70
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26,72345,3806,18153,2914,3486,1643,618,1491,192,26,36,24,30,28,24,34,27,25,27,32,33,28,33,25,29,46,34,33,29,26,26,24,31,27,27,37,28,26,30,26,27,25,25,29,24,36,25,37,25,34,52940,34556,17094,2742,377,218,310,1186,1777,30,30,32,26,27,26,30,23,37,33,34,25,25,25,25,34,28,37,26,32,26,35,32,41,32,23,32,23,35,35,38,22,27,32,38,25,32,40,32,25,24,27,29,25,26,38,25,22,24,26,26,35,29,25,25,42,30,28,39,37,25

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26,72345,3806,18153,2914,3486,1643,618,1491,192,26,36,24,30,28,24,34,27,25,27,32,33,28,33,25,29,46,34,33,29,26,26,24,31,27,27,37,28,26,30,26,27,25,25,29,24,36,25,37,25,34,52940,34556,17094,2742,377,218,310,1186,1777,30,30,32,26,27,26,30,23,37,33,34,25,25,25,25,34,28,37,26,32,26,35,32,41,32,23,32,23,35,35,38,22,27,32,38,25,32,40,32,25,24,27,29,25,26,38,25,22,24,26,26,35,29,25,25,42,30,28,39,37,25
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,79,7,9,19,14,13,16,17,22,25,8,5,6,10,12,14,14,31,32,50,57,63,66,72,85,86,87,95,42,52,53,59,57,57,59,56,46,49,54,53,6,6,8,10,13,17,22,23,25,24,30,25,28,29,36,34,33,32,45,46,41,36,37,40,43,45,50,68,77,77,79,84,83,77,71,77,62,67,64,64,73,95,100,98,95,81,82,91,86,84,5,5,9,13,14,18,20,18,17,18,19,21,25,25,28,27,28,34,34,37,36,37,38,41,44,45,49,51,51,55,50,50,56,59,59,59,63,61,63,72,64,64,70,75,73,78,80,76,88,84,88,86,87,91,90,88,97,96,91,104,97,106,102,105,102,102,114,104,115,108
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration Rewards,-106421.0,-9166.0,-23570.0,-10739.0,-1683.0,-215.0,-1271.0,-870.0,45.0,71.0,-28524.0,-5804.0,-6699.0,-9457.0,-1178.0,-437.0,-227.0,-385.0,-182.0,75.0,65.0,77.0,79.0,79.0,74.0,69.0,70.0,73.0,79.0,67.0,65.0,80.0,66.0,70.0,72.0,71.0,63.0,73.0,70.0,76.0,-72243.0,-3704.0,-18051.0,-2812.0,-3384.0,-1541.0,-516.0,-1389.0,-90.0,76.0,66.0,78.0,72.0,74.0,78.0,68.0,75.0,77.0,75.0,70.0,69.0,74.0,69.0,77.0,73.0,56.0,68.0,69.0,73.0,76.0,76.0,78.0,71.0,75.0,75.0,65.0,74.0,76.0,72.0,76.0,75.0,77.0,77.0,73.0,78.0,66.0,77.0,65.0,77.0,68.0,-52838.0,-34454.0,-16992.0,-2640.0,-275.0,-116.0,-208.0,-1084.0,-1675.0,72.0,72.0,70.0,76.0,75.0,76.0,72.0,79.0,65.0,69.0,68.0,77.0,77.0,77.0,77.0,68.0,74.0,65.0,76.0,70.0,76.0,67.0,70.0,61.0,70.0,79.0,70.0,79.0,67.0,67.0,64.0,80.0,75.0,70.0,64.0,77.0,70.0,62.0,70.0,77.0,78.0,75.0,73.0,77.0,76.0,64.0,77.0,80.0,78.0,76.0,76.0,67.0,73.0,77.0,77.0,60.0,72.0,74.0,63.0,65.0,77.0
Policy Iteration Rewards,
Q Learning Rewards,
Running 90
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Value Iteration Analysis//
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26,72345,3806,18153,2914,3486,1643,618,1491,192,26,36,24,30,28,24,34,27,25,27,32,33,28,33,25,29,46,34,33,29,26,26,24,31,27,27,37,28,26,30,26,27,25,25,29,24,36,25,37,25,34,52940,34556,17094,2742,377,218,310,1186,1777,30,30,32,26,27,26,30,23,37,33,34,25,25,25,25,34,28,37,26,32,26,35,32,41,32,23,32,23,35,35,38,22,27,32,38,25,32,40,32,25,24,27,29,25,26,38,25,22,24,26,26,35,29,25,25,42,30,28,39,37,25,106291,46954,6335,3298,873,588,702,1205,753,34,28,23,35,34,21,31,26,38,24,25,25,29,38,23,25,30,33,30,32,28,28,22,30,27,25,28,24,54,29,29,30,22,32,35,28,30,29,28,40,30,33,29,24,27,30,37,26,33,22,25,26,32,32,28,31,22,23,33,33,34,31,35,24,28,29,28,29,23,31,29,25,23,33,38,26,29,31,30,23,24

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,^,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration,106523,9268,23672,10841,1785,317,1373,972,57,31,28626,5906,6801,9559,1280,539,329,487,284,27,37,25,23,23,28,33,32,29,23,35,37,22,36,32,30,31,39,29,32,26,72345,3806,18153,2914,3486,1643,618,1491,192,26,36,24,30,28,24,34,27,25,27,32,33,28,33,25,29,46,34,33,29,26,26,24,31,27,27,37,28,26,30,26,27,25,25,29,24,36,25,37,25,34,52940,34556,17094,2742,377,218,310,1186,1777,30,30,32,26,27,26,30,23,37,33,34,25,25,25,25,34,28,37,26,32,26,35,32,41,32,23,32,23,35,35,38,22,27,32,38,25,32,40,32,25,24,27,29,25,26,38,25,22,24,26,26,35,29,25,25,42,30,28,39,37,25,106291,46954,6335,3298,873,588,702,1205,753,34,28,23,35,34,21,31,26,38,24,25,25,29,38,23,25,30,33,30,32,28,28,22,30,27,25,28,24,54,29,29,30,22,32,35,28,30,29,28,40,30,33,29,24,27,30,37,26,33,22,25,26,32,32,28,31,22,23,33,33,34,31,35,24,28,29,28,29,23,31,29,25,23,33,38,26,29,31,30,23,24
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration,79,7,9,19,14,13,16,17,22,25,8,5,6,10,12,14,14,31,32,50,57,63,66,72,85,86,87,95,42,52,53,59,57,57,59,56,46,49,54,53,6,6,8,10,13,17,22,23,25,24,30,25,28,29,36,34,33,32,45,46,41,36,37,40,43,45,50,68,77,77,79,84,83,77,71,77,62,67,64,64,73,95,100,98,95,81,82,91,86,84,5,5,9,13,14,18,20,18,17,18,19,21,25,25,28,27,28,34,34,37,36,37,38,41,44,45,49,51,51,55,50,50,56,59,59,59,63,61,63,72,64,64,70,75,73,78,80,76,88,84,88,86,87,91,90,88,97,96,91,104,97,106,102,105,102,102,114,104,115,108,6,6,6,9,9,12,15,17,17,18,23,22,24,28,30,30,49,34,33,34,36,35,40,42,41,45,43,48,45,47,50,55,51,58,57,64,57,62,59,61,62,71,71,86,88,81,84,87,84,92,130,114,94,84,90,86,94,90,99,111,110,109,110,111,139,140,143,140,148,156,157,162,170,181,191,143,193,168,181,189,159,144,144,161,136,166,155,142,163,176
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration Rewards,-106421.0,-9166.0,-23570.0,-10739.0,-1683.0,-215.0,-1271.0,-870.0,45.0,71.0,-28524.0,-5804.0,-6699.0,-9457.0,-1178.0,-437.0,-227.0,-385.0,-182.0,75.0,65.0,77.0,79.0,79.0,74.0,69.0,70.0,73.0,79.0,67.0,65.0,80.0,66.0,70.0,72.0,71.0,63.0,73.0,70.0,76.0,-72243.0,-3704.0,-18051.0,-2812.0,-3384.0,-1541.0,-516.0,-1389.0,-90.0,76.0,66.0,78.0,72.0,74.0,78.0,68.0,75.0,77.0,75.0,70.0,69.0,74.0,69.0,77.0,73.0,56.0,68.0,69.0,73.0,76.0,76.0,78.0,71.0,75.0,75.0,65.0,74.0,76.0,72.0,76.0,75.0,77.0,77.0,73.0,78.0,66.0,77.0,65.0,77.0,68.0,-52838.0,-34454.0,-16992.0,-2640.0,-275.0,-116.0,-208.0,-1084.0,-1675.0,72.0,72.0,70.0,76.0,75.0,76.0,72.0,79.0,65.0,69.0,68.0,77.0,77.0,77.0,77.0,68.0,74.0,65.0,76.0,70.0,76.0,67.0,70.0,61.0,70.0,79.0,70.0,79.0,67.0,67.0,64.0,80.0,75.0,70.0,64.0,77.0,70.0,62.0,70.0,77.0,78.0,75.0,73.0,77.0,76.0,64.0,77.0,80.0,78.0,76.0,76.0,67.0,73.0,77.0,77.0,60.0,72.0,74.0,63.0,65.0,77.0,-106189.0,-46852.0,-6233.0,-3196.0,-771.0,-486.0,-600.0,-1103.0,-651.0,68.0,74.0,79.0,67.0,68.0,81.0,71.0,76.0,64.0,78.0,77.0,77.0,73.0,64.0,79.0,77.0,72.0,69.0,72.0,70.0,74.0,74.0,80.0,72.0,75.0,77.0,74.0,78.0,48.0,73.0,73.0,72.0,80.0,70.0,67.0,74.0,72.0,73.0,74.0,62.0,72.0,69.0,73.0,78.0,75.0,72.0,65.0,76.0,69.0,80.0,77.0,76.0,70.0,70.0,74.0,71.0,80.0,79.0,69.0,69.0,68.0,71.0,67.0,78.0,74.0,73.0,74.0,73.0,79.0,71.0,73.0,77.0,79.0,69.0,64.0,76.0,73.0,71.0,72.0,79.0,78.0
Policy Iteration Rewards,
Q Learning Rewards,
