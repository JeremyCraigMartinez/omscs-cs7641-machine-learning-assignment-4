Running 10
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,605,395,139,502,370,209,154,300,147,126
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,<,^,<,>,<,>,*,*,>,v,>,*]
[*,*,*,*,*,<,<,>,<,v,^,*,*,>,^,v,<]
[*,*,*,*,*,>,v,v,<,v,v,v,v,<,v,<,>]
[*,*,*,*,*,*,<,>,>,>,v,<,<,v,^,^,v]
[*,*,*,*,*,*,<,^,>,<,*,v,<,<,<,^,v]
[*,*,*,*,*,*,v,<,>,<,*,*,^,v,^,^,<]
[*,*,*,*,*,>,v,<,>,^,*,*,*,v,<,^,v]
[*,*,*,*,*,v,^,<,v,v,*,*,*,*,*,<,<]
[*,*,*,*,v,<,>,<,<,<,*,*,*,*,*,*,>]
[*,*,*,*,^,v,v,v,v,>,*,*,*,*,*,<,^]
[v,*,*,<,v,v,^,v,^,*,*,*,*,*,*,>,^]
[v,<,^,^,>,<,^,^,*,*,*,*,*,*,*,v,>]
[<,v,^,v,<,<,v,^,*,*,*,*,*,*,*,^,^]
[<,<,v,^,^,^,^,*,*,*,*,*,*,*,<,<,<]
[v,<,>,<,v,<,<,*,*,*,*,*,*,*,^,>,v]
[v,>,v,^,<,<,>,*,*,*,*,*,*,*,>,^,<]
[v,>,<,>,^,^,^,*,*,*,*,*,*,*,>,v,v]
[^,^,>,>,>,>,>,*,*,*,*,*,*,*,<,v,>]
[^,v,<,>,^,>,<,*,*,*,*,*,*,*,v,v,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,
Q Learning,605,395,139,502,370,209,154,300,147,126

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,
Q Learning,78,33,20,37,7,7,7,19,10,7

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-503.0,-293.0,-37.0,-400.0,-268.0,-107.0,-52.0,-198.0,-45.0,-24.0

Running 30
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,355,330,194,142,191,206,109,146,76,76,140,94,98,56,43,122,156,166,141,27,86,56,56,55,60,44,91,221,592,158
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,^,v,v,<,<,<,*,*,v,v,v,*]
[*,*,*,*,*,<,>,<,>,v,^,*,*,<,<,<,<]
[*,*,*,*,*,<,v,>,^,>,v,v,<,<,<,^,<]
[*,*,*,*,*,*,>,<,>,<,v,<,v,^,^,<,<]
[*,*,*,*,*,*,^,v,^,v,*,^,<,<,v,>,^]
[*,*,*,*,*,*,>,>,>,<,*,*,^,<,<,v,<]
[*,*,*,*,*,v,<,>,<,<,*,*,*,^,>,^,^]
[*,*,*,*,*,v,^,^,v,v,*,*,*,*,*,^,<]
[*,*,*,*,^,v,^,v,>,<,*,*,*,*,*,*,>]
[*,*,*,*,v,>,>,<,>,<,*,*,*,*,*,v,<]
[^,*,*,>,>,v,v,>,>,*,*,*,*,*,*,<,^]
[v,v,>,v,v,>,^,^,*,*,*,*,*,*,*,>,^]
[v,v,>,<,>,>,^,<,*,*,*,*,*,*,*,^,^]
[^,<,>,^,<,<,>,*,*,*,*,*,*,*,>,^,^]
[^,^,^,>,v,^,>,*,*,*,*,*,*,*,>,^,^]
[<,<,>,v,<,>,^,*,*,*,*,*,*,*,<,^,^]
[v,>,<,>,>,v,>,*,*,*,*,*,*,*,^,^,v]
[<,>,>,^,>,v,>,*,*,*,*,*,*,*,^,v,^]
[^,v,v,^,<,<,>,*,*,*,*,*,*,*,<,v,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,
Policy Iteration,
Q Learning,355,330,194,142,191,206,109,146,76,76,140,94,98,56,43,122,156,166,141,27,86,56,56,55,60,44,91,221,592,158

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,
Policy Iteration,
Q Learning,71,18,20,18,29,13,7,7,8,15,8,8,10,11,8,17,14,10,12,28,40,18,10,10,14,18,13,12,13,18

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-253.0,-228.0,-92.0,-40.0,-89.0,-104.0,-7.0,-44.0,26.0,26.0,-38.0,8.0,4.0,46.0,59.0,-20.0,-54.0,-64.0,-39.0,75.0,16.0,46.0,46.0,47.0,42.0,58.0,11.0,-119.0,-490.0,-56.0

Running 70
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,334,179,55,127,381,324,141,60,183,54,49,111,81,197,59,41,110,107,112,46,89,279,73,42,70,175,44,148,36,197,40,77,36,111,97,132,40,59,47,337,53,99,76,45,44,128,125,178,31,70,39,111,33,33,61,38,78,61,110,61,71,33,55,40,64,47,74,29,107,63
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,<,<,>,>,v,^,*,*,>,^,<,*]
[*,*,*,*,*,>,>,v,^,>,>,*,*,v,v,v,<]
[*,*,*,*,*,<,^,^,v,<,v,>,^,v,v,v,<]
[*,*,*,*,*,*,<,^,^,<,v,<,^,<,v,v,<]
[*,*,*,*,*,*,<,>,>,>,*,^,<,<,>,v,^]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,<,v,v,v,*,*,*,^,<,^,<]
[*,*,*,*,*,<,^,v,^,<,*,*,*,*,*,^,v]
[*,*,*,*,<,>,<,v,>,<,*,*,*,*,*,*,^]
[*,*,*,*,<,<,>,^,v,>,*,*,*,*,*,^,>]
[^,*,*,>,^,>,^,v,v,*,*,*,*,*,*,^,>]
[>,>,<,<,>,^,<,>,*,*,*,*,*,*,*,^,^]
[^,v,>,^,<,^,<,<,*,*,*,*,*,*,*,^,^]
[<,>,^,v,>,>,<,*,*,*,*,*,*,*,v,>,^]
[v,^,<,^,>,^,^,*,*,*,*,*,*,*,>,>,^]
[v,^,^,>,v,>,v,*,*,*,*,*,*,*,>,^,^]
[<,^,<,>,>,v,<,*,*,*,*,*,*,*,>,>,^]
[^,^,<,^,^,<,^,*,*,*,*,*,*,*,>,>,>]
[>,>,v,<,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,
Policy Iteration,
Q Learning,334,179,55,127,381,324,141,60,183,54,49,111,81,197,59,41,110,107,112,46,89,279,73,42,70,175,44,148,36,197,40,77,36,111,97,132,40,59,47,337,53,99,76,45,44,128,125,178,31,70,39,111,33,33,61,38,78,61,110,61,71,33,55,40,64,47,74,29,107,63

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,
Policy Iteration,
Q Learning,74,19,23,16,24,11,11,11,20,8,9,8,11,13,8,19,14,12,26,31,26,18,11,13,17,24,10,11,12,20,49,43,40,26,16,15,17,18,12,15,20,14,21,19,14,19,23,39,58,51,51,62,20,13,21,13,21,16,17,17,16,20,17,16,22,18,16,19,24,15

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-232.0,-77.0,47.0,-25.0,-279.0,-222.0,-39.0,42.0,-81.0,48.0,53.0,-9.0,21.0,-95.0,43.0,61.0,-8.0,-5.0,-10.0,56.0,13.0,-177.0,29.0,60.0,32.0,-73.0,58.0,-46.0,66.0,-95.0,62.0,25.0,66.0,-9.0,5.0,-30.0,62.0,43.0,55.0,-235.0,49.0,3.0,26.0,57.0,58.0,-26.0,-23.0,-76.0,71.0,32.0,63.0,-9.0,69.0,69.0,41.0,64.0,24.0,41.0,-8.0,41.0,31.0,69.0,47.0,62.0,38.0,55.0,28.0,73.0,-5.0,39.0

Running 140
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,122,456,213,167,91,150,177,88,190,129,327,121,127,66,60,117,133,57,43,43,92,109,59,358,75,110,88,39,42,89,105,98,37,137,58,40,36,124,121,40,55,496,76,207,81,69,54,144,42,47,48,113,57,106,49,285,239,76,41,46,78,105,81,86,80,89,37,53,49,57,71,165,54,91,72,42,61,232,65,53,44,65,69,104,55,57,69,75,46,43,49,94,104,56,69,180,92,67,80,44,52,31,37,101,31,55,40,78,146,62,51,140,60,242,213,61,30,50,50,71,60,58,32,63,90,46,43,44,63,60,41,138,73,61,44,45,43,33,39,34
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,<,>,^,<,>,<,*,*,v,>,v,*]
[*,*,*,*,*,v,v,^,<,<,>,*,*,v,<,^,>]
[*,*,*,*,*,v,v,<,>,>,v,^,v,<,<,v,<]
[*,*,*,*,*,*,^,^,^,>,v,<,<,<,v,<,<]
[*,*,*,*,*,*,>,^,<,<,*,^,^,>,^,v,v]
[*,*,*,*,*,*,v,^,>,>,*,*,^,<,<,<,<]
[*,*,*,*,*,^,>,>,^,^,*,*,*,^,^,^,v]
[*,*,*,*,*,>,<,<,<,^,*,*,*,*,*,^,<]
[*,*,*,*,>,v,<,<,>,v,*,*,*,*,*,*,^]
[*,*,*,*,v,<,<,v,^,>,*,*,*,*,*,v,v]
[>,*,*,>,>,v,>,^,>,*,*,*,*,*,*,>,^]
[^,v,v,>,v,<,^,<,*,*,*,*,*,*,*,^,>]
[^,v,v,>,<,>,^,v,*,*,*,*,*,*,*,^,^]
[v,^,v,^,v,^,v,*,*,*,*,*,*,*,>,>,^]
[>,>,v,^,<,v,>,*,*,*,*,*,*,*,^,^,^]
[<,<,^,>,^,>,<,*,*,*,*,*,*,*,^,^,^]
[<,^,>,<,^,v,<,*,*,*,*,*,*,*,>,>,^]
[v,v,<,<,<,>,>,*,*,*,*,*,*,*,^,>,^]
[>,v,<,<,<,<,^,*,*,*,*,*,*,*,^,<,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140
Value Iteration,
Policy Iteration,
Q Learning,122,456,213,167,91,150,177,88,190,129,327,121,127,66,60,117,133,57,43,43,92,109,59,358,75,110,88,39,42,89,105,98,37,137,58,40,36,124,121,40,55,496,76,207,81,69,54,144,42,47,48,113,57,106,49,285,239,76,41,46,78,105,81,86,80,89,37,53,49,57,71,165,54,91,72,42,61,232,65,53,44,65,69,104,55,57,69,75,46,43,49,94,104,56,69,180,92,67,80,44,52,31,37,101,31,55,40,78,146,62,51,140,60,242,213,61,30,50,50,71,60,58,32,63,90,46,43,44,63,60,41,138,73,61,44,45,43,33,39,34

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140
Value Iteration,
Policy Iteration,
Q Learning,60,24,32,13,18,26,8,10,11,17,8,10,13,7,11,15,8,7,10,27,30,33,13,15,18,18,13,9,11,10,20,29,31,39,14,17,14,13,14,15,13,12,11,17,14,17,17,39,44,42,41,46,33,24,20,15,21,14,15,17,20,16,16,17,15,23,18,17,22,22,22,29,23,22,52,55,53,65,66,41,22,25,24,23,28,21,21,26,21,28,22,23,19,20,20,24,21,34,18,22,27,26,27,34,31,46,52,68,58,82,77,69,73,30,31,24,30,21,30,19,29,31,27,33,30,29,25,31,24,30,26,29,30,32,25,32,30,32,31,24

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-20.0,-354.0,-111.0,-65.0,11.0,-48.0,-75.0,14.0,-88.0,-27.0,-225.0,-19.0,-25.0,36.0,42.0,-15.0,-31.0,45.0,59.0,59.0,10.0,-7.0,43.0,-256.0,27.0,-8.0,14.0,63.0,60.0,13.0,-3.0,4.0,65.0,-35.0,44.0,62.0,66.0,-22.0,-19.0,62.0,47.0,-394.0,26.0,-105.0,21.0,33.0,48.0,-42.0,60.0,55.0,54.0,-11.0,45.0,-4.0,53.0,-183.0,-137.0,26.0,61.0,56.0,24.0,-3.0,21.0,16.0,22.0,13.0,65.0,49.0,53.0,45.0,31.0,-63.0,48.0,11.0,30.0,60.0,41.0,-130.0,37.0,49.0,58.0,37.0,33.0,-2.0,47.0,45.0,33.0,27.0,56.0,59.0,53.0,8.0,-2.0,46.0,33.0,-78.0,10.0,35.0,22.0,58.0,50.0,71.0,65.0,1.0,71.0,47.0,62.0,24.0,-44.0,40.0,51.0,-38.0,42.0,-140.0,-111.0,41.0,72.0,52.0,52.0,31.0,42.0,44.0,70.0,39.0,12.0,56.0,59.0,58.0,39.0,42.0,61.0,-36.0,29.0,41.0,58.0,57.0,59.0,69.0,63.0,68.0

Running 190
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,361,566,299,261,269,692,179,202,113,79,126,241,109,201,85,35,85,611,25,144,102,80,105,65,117,69,46,84,101,77,49,233,34,101,77,77,89,40,42,136,205,48,57,70,66,37,70,93,40,44,83,31,53,42,60,34,34,38,36,40,77,278,26,54,329,60,37,62,84,124,132,66,151,85,100,50,158,60,55,109,67,56,140,1332,71,48,48,40,36,35,90,71,57,90,105,80,58,66,43,32,109,56,89,95,196,42,102,49,99,53,43,87,30,134,46,65,29,184,69,55,47,41,42,63,210,227,70,91,44,36,45,36,47,55,69,114,85,55,31,34,33,26,80,36,45,37,65,55,40,44,41,53,58,61,67,34,50,132,49,41,68,65,72,34,24,56,67,37,116,73,118,95,33,82,42,82,71,47,56,73,40,39,101,459,37,95,85,43,135,90
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,^,v,>,^,>,*,*,>,v,<,*]
[*,*,*,*,*,>,>,^,^,>,v,*,*,^,v,<,^]
[*,*,*,*,*,v,<,<,>,v,v,^,^,v,v,^,<]
[*,*,*,*,*,*,v,<,>,>,v,<,<,<,<,<,^]
[*,*,*,*,*,*,<,^,>,^,*,^,<,^,<,<,v]
[*,*,*,*,*,*,<,<,>,<,*,*,^,>,^,<,^]
[*,*,*,*,*,>,>,<,v,^,*,*,*,>,^,<,<]
[*,*,*,*,*,^,<,v,<,<,*,*,*,*,*,^,^]
[*,*,*,*,<,^,^,v,v,v,*,*,*,*,*,*,^]
[*,*,*,*,^,>,v,>,v,^,*,*,*,*,*,>,^]
[<,*,*,>,<,>,^,v,<,*,*,*,*,*,*,^,^]
[>,^,<,v,^,v,v,>,*,*,*,*,*,*,*,<,^]
[<,>,<,<,>,<,v,<,*,*,*,*,*,*,*,^,^]
[v,v,<,v,>,>,^,*,*,*,*,*,*,*,v,v,^]
[<,v,v,>,<,<,<,*,*,*,*,*,*,*,>,>,<]
[>,>,v,<,>,^,v,*,*,*,*,*,*,*,^,^,^]
[<,^,v,^,^,<,<,*,*,*,*,*,*,*,<,^,^]
[>,^,v,v,^,^,^,*,*,*,*,*,*,*,^,^,<]
[v,^,^,^,<,v,>,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190
Value Iteration,
Policy Iteration,
Q Learning,361,566,299,261,269,692,179,202,113,79,126,241,109,201,85,35,85,611,25,144,102,80,105,65,117,69,46,84,101,77,49,233,34,101,77,77,89,40,42,136,205,48,57,70,66,37,70,93,40,44,83,31,53,42,60,34,34,38,36,40,77,278,26,54,329,60,37,62,84,124,132,66,151,85,100,50,158,60,55,109,67,56,140,1332,71,48,48,40,36,35,90,71,57,90,105,80,58,66,43,32,109,56,89,95,196,42,102,49,99,53,43,87,30,134,46,65,29,184,69,55,47,41,42,63,210,227,70,91,44,36,45,36,47,55,69,114,85,55,31,34,33,26,80,36,45,37,65,55,40,44,41,53,58,61,67,34,50,132,49,41,68,65,72,34,24,56,67,37,116,73,118,95,33,82,42,82,71,47,56,73,40,39,101,459,37,95,85,43,135,90

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190
Value Iteration,
Policy Iteration,
Q Learning,78,29,20,16,19,9,7,9,13,8,10,8,13,12,9,14,8,12,11,26,34,15,12,13,23,12,18,11,15,24,44,37,27,22,19,14,13,14,13,17,11,12,19,17,13,28,44,47,44,38,60,16,15,19,13,16,13,16,14,17,21,18,21,19,22,13,13,19,20,19,21,21,19,55,53,55,62,60,40,24,30,20,20,26,29,29,24,24,21,24,27,21,25,25,24,29,24,21,24,21,23,25,23,32,29,68,76,60,60,82,67,65,44,24,38,39,25,29,33,31,29,30,30,34,29,28,40,41,27,29,30,31,31,28,28,28,35,29,37,33,26,35,34,29,34,49,32,49,31,44,45,32,46,32,34,29,51,39,41,30,40,40,37,30,37,30,40,45,34,48,41,37,41,44,43,35,45,39,42,33,39,42,32,39,36,45,43,52,37,44

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-259.0,-464.0,-197.0,-159.0,-167.0,-590.0,-77.0,-100.0,-11.0,23.0,-24.0,-139.0,-7.0,-99.0,17.0,67.0,17.0,-509.0,77.0,-42.0,0.0,22.0,-3.0,37.0,-15.0,33.0,56.0,18.0,1.0,25.0,53.0,-131.0,68.0,1.0,25.0,25.0,13.0,62.0,60.0,-34.0,-103.0,54.0,45.0,32.0,36.0,65.0,32.0,9.0,62.0,58.0,19.0,71.0,49.0,60.0,42.0,68.0,68.0,64.0,66.0,62.0,25.0,-176.0,76.0,48.0,-227.0,42.0,65.0,40.0,18.0,-22.0,-30.0,36.0,-49.0,17.0,2.0,52.0,-56.0,42.0,47.0,-7.0,35.0,46.0,-38.0,-1230.0,31.0,54.0,54.0,62.0,66.0,67.0,12.0,31.0,45.0,12.0,-3.0,22.0,44.0,36.0,59.0,70.0,-7.0,46.0,13.0,7.0,-94.0,60.0,0.0,53.0,3.0,49.0,59.0,15.0,72.0,-32.0,56.0,37.0,73.0,-82.0,33.0,47.0,55.0,61.0,60.0,39.0,-108.0,-125.0,32.0,11.0,58.0,66.0,57.0,66.0,55.0,47.0,33.0,-12.0,17.0,47.0,71.0,68.0,69.0,76.0,22.0,66.0,57.0,65.0,37.0,47.0,62.0,58.0,61.0,49.0,44.0,41.0,35.0,68.0,52.0,-30.0,53.0,61.0,34.0,37.0,30.0,68.0,78.0,46.0,35.0,65.0,-14.0,29.0,-16.0,7.0,69.0,20.0,60.0,20.0,31.0,55.0,46.0,29.0,62.0,63.0,1.0,-357.0,65.0,7.0,17.0,59.0,-33.0,12.0

Running 250
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,509,407,891,186,375,256,188,59,217,58,211,192,91,124,255,38,77,59,39,64,42,34,47,131,114,84,52,66,79,90,142,70,201,137,135,47,52,54,57,83,112,81,49,70,52,37,55,43,37,55,130,41,104,44,48,43,185,138,62,72,39,74,104,68,63,29,48,37,51,105,88,77,64,67,40,51,79,107,40,176,86,78,63,121,38,63,101,142,58,117,51,36,25,237,38,65,44,79,34,64,70,119,146,53,75,69,56,34,53,30,79,57,51,105,64,82,57,102,76,46,99,46,65,24,44,31,91,71,35,42,85,66,31,32,64,44,45,49,71,85,45,117,57,82,60,78,102,75,61,107,36,50,35,90,60,103,37,28,117,75,229,63,89,56,59,62,45,57,69,36,51,62,79,37,200,71,46,77,106,50,80,54,166,28,40,29,76,42,61,105,52,40,92,63,32,52,44,99,208,67,35,54,61,83,97,110,40,84,68,53,33,40,39,32,60,74,86,60,28,25,69,56,84,56,45,54,61,50,93,73,26,83,42,142,148,69,37,32,30,69,94,67,34,57,44,61,147,70,43,81
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,^,v,>,v,<,v,*,*,v,<,<,*]
[*,*,*,*,*,^,<,>,^,v,v,*,*,<,<,v,<]
[*,*,*,*,*,^,<,^,v,^,v,<,v,v,<,<,<]
[*,*,*,*,*,*,^,>,>,>,^,<,>,<,<,v,<]
[*,*,*,*,*,*,v,v,^,v,*,^,<,^,v,>,<]
[*,*,*,*,*,*,v,<,<,^,*,*,<,^,>,^,v]
[*,*,*,*,*,>,v,>,<,>,*,*,*,^,>,<,<]
[*,*,*,*,*,^,>,>,>,>,*,*,*,*,*,^,<]
[*,*,*,*,>,^,<,v,<,v,*,*,*,*,*,*,^]
[*,*,*,*,^,v,<,^,^,>,*,*,*,*,*,>,^]
[<,*,*,<,<,>,>,^,^,*,*,*,*,*,*,>,^]
[<,v,>,^,v,v,<,^,*,*,*,*,*,*,*,^,>]
[>,v,v,<,>,<,v,v,*,*,*,*,*,*,*,>,^]
[>,^,v,v,<,>,>,*,*,*,*,*,*,*,>,^,^]
[^,^,v,<,v,>,>,*,*,*,*,*,*,*,^,>,^]
[v,v,<,<,v,v,v,*,*,*,*,*,*,*,^,^,^]
[<,>,v,v,^,>,>,*,*,*,*,*,*,*,>,^,^]
[v,v,v,>,v,>,<,*,*,*,*,*,*,*,>,>,^]
[<,v,<,<,^,^,<,*,*,*,*,*,*,*,>,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250
Value Iteration,
Policy Iteration,
Q Learning,509,407,891,186,375,256,188,59,217,58,211,192,91,124,255,38,77,59,39,64,42,34,47,131,114,84,52,66,79,90,142,70,201,137,135,47,52,54,57,83,112,81,49,70,52,37,55,43,37,55,130,41,104,44,48,43,185,138,62,72,39,74,104,68,63,29,48,37,51,105,88,77,64,67,40,51,79,107,40,176,86,78,63,121,38,63,101,142,58,117,51,36,25,237,38,65,44,79,34,64,70,119,146,53,75,69,56,34,53,30,79,57,51,105,64,82,57,102,76,46,99,46,65,24,44,31,91,71,35,42,85,66,31,32,64,44,45,49,71,85,45,117,57,82,60,78,102,75,61,107,36,50,35,90,60,103,37,28,117,75,229,63,89,56,59,62,45,57,69,36,51,62,79,37,200,71,46,77,106,50,80,54,166,28,40,29,76,42,61,105,52,40,92,63,32,52,44,99,208,67,35,54,61,83,97,110,40,84,68,53,33,40,39,32,60,74,86,60,28,25,69,56,84,56,45,54,61,50,93,73,26,83,42,142,148,69,37,32,30,69,94,67,34,57,44,61,147,70,43,81

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250
Value Iteration,
Policy Iteration,
Q Learning,72,20,27,17,29,8,9,21,9,11,10,16,13,14,11,9,9,13,28,31,12,10,13,11,13,18,13,15,16,46,36,32,28,16,11,14,11,13,13,11,16,20,17,21,23,18,20,34,40,59,50,38,43,13,14,15,16,19,21,21,18,16,23,21,17,18,18,15,16,17,19,18,24,18,32,44,55,50,61,58,34,21,19,21,18,28,21,28,24,23,28,24,24,23,26,21,18,29,32,34,32,29,24,28,21,68,75,59,61,94,86,84,31,31,24,22,38,28,29,27,25,27,24,29,26,29,33,33,34,35,38,27,31,31,30,30,49,40,38,29,32,36,30,37,51,25,31,29,33,35,47,26,27,44,40,49,35,35,39,35,34,38,31,29,33,28,31,31,41,36,41,31,40,31,40,42,42,59,40,47,41,35,42,41,42,45,39,38,41,36,39,33,40,53,40,38,41,37,40,43,38,45,46,42,51,40,44,46,47,47,46,47,49,48,39,50,43,47,42,39,49,61,38,44,51,42,47,51,48,56,47,42,50,51,43,52,51,47,55,45,42,44,53,47,67,43,52,52,49,50

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-407.0,-305.0,-789.0,-84.0,-273.0,-154.0,-86.0,43.0,-115.0,44.0,-109.0,-90.0,11.0,-22.0,-153.0,64.0,25.0,43.0,63.0,38.0,60.0,68.0,55.0,-29.0,-12.0,18.0,50.0,36.0,23.0,12.0,-40.0,32.0,-99.0,-35.0,-33.0,55.0,50.0,48.0,45.0,19.0,-10.0,21.0,53.0,32.0,50.0,65.0,47.0,59.0,65.0,47.0,-28.0,61.0,-2.0,58.0,54.0,59.0,-83.0,-36.0,40.0,30.0,63.0,28.0,-2.0,34.0,39.0,73.0,54.0,65.0,51.0,-3.0,14.0,25.0,38.0,35.0,62.0,51.0,23.0,-5.0,62.0,-74.0,16.0,24.0,39.0,-19.0,64.0,39.0,1.0,-40.0,44.0,-15.0,51.0,66.0,77.0,-135.0,64.0,37.0,58.0,23.0,68.0,38.0,32.0,-17.0,-44.0,49.0,27.0,33.0,46.0,68.0,49.0,72.0,23.0,45.0,51.0,-3.0,38.0,20.0,45.0,0.0,26.0,56.0,3.0,56.0,37.0,78.0,58.0,71.0,11.0,31.0,67.0,60.0,17.0,36.0,71.0,70.0,38.0,58.0,57.0,53.0,31.0,17.0,57.0,-15.0,45.0,20.0,42.0,24.0,0.0,27.0,41.0,-5.0,66.0,52.0,67.0,12.0,42.0,-1.0,65.0,74.0,-15.0,27.0,-127.0,39.0,13.0,46.0,43.0,40.0,57.0,45.0,33.0,66.0,51.0,40.0,23.0,65.0,-98.0,31.0,56.0,25.0,-4.0,52.0,22.0,48.0,-64.0,74.0,62.0,73.0,26.0,60.0,41.0,-3.0,50.0,62.0,10.0,39.0,70.0,50.0,58.0,3.0,-106.0,35.0,67.0,48.0,41.0,19.0,5.0,-8.0,62.0,18.0,34.0,49.0,69.0,62.0,63.0,70.0,42.0,28.0,16.0,42.0,74.0,77.0,33.0,46.0,18.0,46.0,57.0,48.0,41.0,52.0,9.0,29.0,76.0,19.0,60.0,-40.0,-46.0,33.0,65.0,70.0,72.0,33.0,8.0,35.0,68.0,45.0,58.0,41.0,-45.0,32.0,59.0,21.0
