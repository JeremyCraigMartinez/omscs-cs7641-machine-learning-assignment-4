Running 10
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,694,441,536,75,133,322,97,114,85,201
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,v,^,<,v,<,*,*,^,^,<,*]
[*,*,*,*,*,<,^,^,^,v,<,*,*,^,^,^,<]
[*,*,*,*,*,<,^,<,v,>,v,^,v,v,^,v,<]
[*,*,*,*,*,*,v,^,>,<,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,v,<,^,*,^,<,^,<,<,<]
[*,*,*,*,*,*,v,v,v,v,*,*,^,<,^,^,^]
[*,*,*,*,*,>,^,<,<,^,*,*,*,^,^,^,^]
[*,*,*,*,*,v,^,v,^,<,*,*,*,*,*,v,v]
[*,*,*,*,<,v,<,>,<,>,*,*,*,*,*,*,^]
[*,*,*,*,<,v,<,v,^,<,*,*,*,*,*,v,^]
[>,*,*,>,^,^,<,^,^,*,*,*,*,*,*,<,^]
[<,<,>,^,<,>,<,v,*,*,*,*,*,*,*,>,^]
[^,^,^,^,^,v,>,v,*,*,*,*,*,*,*,v,<]
[^,<,^,^,>,v,>,*,*,*,*,*,*,*,v,v,<]
[^,>,^,<,^,>,>,*,*,*,*,*,*,*,<,v,<]
[>,>,v,>,v,>,^,*,*,*,*,*,*,*,^,<,>]
[>,>,>,>,>,<,^,*,*,*,*,*,*,*,v,>,^]
[^,>,^,v,^,v,v,*,*,*,*,*,*,*,>,>,^]
[^,v,v,<,v,v,v,*,*,*,*,*,*,*,>,v,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,
Q Learning,694,441,536,75,133,322,97,114,85,201

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,
Q Learning,89,16,25,26,28,18,12,6,18,7

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-592.0,-339.0,-434.0,27.0,-31.0,-220.0,5.0,-12.0,17.0,-99.0

Running 30
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,547,224,292,301,211,165,208,174,239,79,57,136,223,89,208,69,72,88,74,845,43,93,70,43,36,49,41,85,138,58
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,<,>,^,<,<,>,*,*,v,<,>,*]
[*,*,*,*,*,<,^,v,<,>,>,*,*,>,v,<,v]
[*,*,*,*,*,<,<,>,^,>,>,v,<,<,<,^,^]
[*,*,*,*,*,*,^,>,^,>,^,<,<,^,v,<,v]
[*,*,*,*,*,*,v,<,^,>,*,^,<,v,>,<,<]
[*,*,*,*,*,*,>,v,>,v,*,*,^,<,>,>,^]
[*,*,*,*,*,v,>,>,v,>,*,*,*,v,<,^,<]
[*,*,*,*,*,<,>,v,>,<,*,*,*,*,*,^,^]
[*,*,*,*,<,>,<,^,>,^,*,*,*,*,*,*,^]
[*,*,*,*,v,^,>,>,v,>,*,*,*,*,*,>,^]
[^,*,*,<,<,<,<,^,v,*,*,*,*,*,*,>,^]
[>,<,v,v,>,<,<,^,*,*,*,*,*,*,*,^,^]
[>,<,<,<,>,>,^,<,*,*,*,*,*,*,*,<,<]
[>,>,<,^,v,^,^,*,*,*,*,*,*,*,>,^,<]
[<,>,^,^,^,>,^,*,*,*,*,*,*,*,>,^,<]
[v,v,>,^,^,v,v,*,*,*,*,*,*,*,^,^,^]
[v,>,<,v,v,v,v,*,*,*,*,*,*,*,^,^,<]
[>,^,<,v,v,v,^,*,*,*,*,*,*,*,^,<,<]
[^,v,v,<,>,<,^,*,*,*,*,*,*,*,>,>,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,
Policy Iteration,
Q Learning,547,224,292,301,211,165,208,174,239,79,57,136,223,89,208,69,72,88,74,845,43,93,70,43,36,49,41,85,138,58

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,
Policy Iteration,
Q Learning,87,11,24,26,30,10,7,10,12,17,9,7,12,12,8,15,11,8,11,23,33,13,11,10,12,13,15,13,13,15

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-445.0,-122.0,-190.0,-199.0,-109.0,-63.0,-106.0,-72.0,-137.0,23.0,45.0,-34.0,-121.0,13.0,-106.0,33.0,30.0,14.0,28.0,-743.0,59.0,9.0,32.0,59.0,66.0,53.0,61.0,17.0,-36.0,44.0

Running 50
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,1590,582,152,410,231,133,40,139,94,208,234,69,86,107,134,186,33,183,55,123,158,72,117,47,134,64,93,43,106,51,68,49,35,44,41,89,81,67,168,76,111,168,181,129,32,71,47,50,34,49
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,<,<,>,<,v,v,*,*,>,v,v,*]
[*,*,*,*,*,<,^,<,^,<,v,*,*,v,>,v,<]
[*,*,*,*,*,v,^,^,<,<,>,<,<,<,<,>,v]
[*,*,*,*,*,*,^,v,>,>,<,<,<,v,<,<,<]
[*,*,*,*,*,*,<,<,>,<,*,^,^,<,<,v,^]
[*,*,*,*,*,*,v,^,<,v,*,*,^,<,<,<,<]
[*,*,*,*,*,>,v,^,v,<,*,*,*,^,<,^,^]
[*,*,*,*,*,^,>,>,^,v,*,*,*,*,*,^,<]
[*,*,*,*,v,<,^,v,v,<,*,*,*,*,*,*,^]
[*,*,*,*,>,<,^,v,v,>,*,*,*,*,*,>,^]
[^,*,*,>,v,<,<,^,v,*,*,*,*,*,*,>,^]
[^,v,v,<,<,^,>,<,*,*,*,*,*,*,*,>,^]
[^,v,v,^,^,>,v,v,*,*,*,*,*,*,*,>,^]
[^,<,>,>,>,<,>,*,*,*,*,*,*,*,<,^,v]
[v,<,>,v,v,v,>,*,*,*,*,*,*,*,^,^,^]
[v,<,^,v,>,^,<,*,*,*,*,*,*,*,^,<,^]
[^,>,v,v,v,<,^,*,*,*,*,*,*,*,^,^,>]
[^,v,>,^,v,>,<,*,*,*,*,*,*,*,>,^,<]
[^,^,^,v,v,^,v,*,*,*,*,*,*,*,^,<,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration,
Policy Iteration,
Q Learning,1590,582,152,410,231,133,40,139,94,208,234,69,86,107,134,186,33,183,55,123,158,72,117,47,134,64,93,43,106,51,68,49,35,44,41,89,81,67,168,76,111,168,181,129,32,71,47,50,34,49

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration,
Policy Iteration,
Q Learning,99,22,15,23,11,11,9,8,18,8,7,10,10,8,8,12,9,12,11,27,31,9,10,9,13,16,15,10,10,10,14,36,36,42,23,14,16,13,14,14,20,16,13,18,16,19,15,16,56,45

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-1488.0,-480.0,-50.0,-308.0,-129.0,-31.0,62.0,-37.0,8.0,-106.0,-132.0,33.0,16.0,-5.0,-32.0,-84.0,69.0,-81.0,47.0,-21.0,-56.0,30.0,-15.0,55.0,-32.0,38.0,9.0,59.0,-4.0,51.0,34.0,53.0,67.0,58.0,61.0,13.0,21.0,35.0,-66.0,26.0,-9.0,-66.0,-79.0,-27.0,70.0,31.0,55.0,52.0,68.0,53.0

Running 70
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,663,153,108,222,112,77,1010,285,358,344,137,78,74,92,121,96,80,58,121,64,37,693,63,45,105,147,36,71,112,90,60,53,59,116,82,40,76,52,96,95,47,130,31,41,243,75,58,293,50,41,87,130,37,51,33,42,54,69,42,39,40,52,43,90,152,64,42,36,33,227
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,<,<,<,<,^,*,*,v,^,<,*]
[*,*,*,*,*,>,v,<,<,v,<,*,*,<,v,<,v]
[*,*,*,*,*,>,>,>,^,^,v,v,v,<,v,v,^]
[*,*,*,*,*,*,<,<,^,>,<,<,>,<,<,<,<]
[*,*,*,*,*,*,>,^,<,v,*,<,^,>,<,<,^]
[*,*,*,*,*,*,v,^,v,^,*,*,<,<,^,>,v]
[*,*,*,*,*,>,<,<,^,<,*,*,*,>,^,<,>]
[*,*,*,*,*,>,v,^,^,>,*,*,*,*,*,^,v]
[*,*,*,*,v,>,>,>,<,^,*,*,*,*,*,*,<]
[*,*,*,*,^,^,v,v,>,^,*,*,*,*,*,>,<]
[v,*,*,<,^,>,v,<,<,*,*,*,*,*,*,^,^]
[<,^,<,^,>,>,v,^,*,*,*,*,*,*,*,v,^]
[<,>,<,v,<,>,>,>,*,*,*,*,*,*,*,<,>]
[>,^,<,v,>,<,>,*,*,*,*,*,*,*,v,>,^]
[^,^,^,<,<,^,>,*,*,*,*,*,*,*,v,v,^]
[^,^,v,v,>,v,^,*,*,*,*,*,*,*,>,>,^]
[<,>,>,<,<,v,^,*,*,*,*,*,*,*,v,<,>]
[v,^,^,v,>,>,v,*,*,*,*,*,*,*,>,>,^]
[^,v,^,<,^,<,<,*,*,*,*,*,*,*,>,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,
Policy Iteration,
Q Learning,663,153,108,222,112,77,1010,285,358,344,137,78,74,92,121,96,80,58,121,64,37,693,63,45,105,147,36,71,112,90,60,53,59,116,82,40,76,52,96,95,47,130,31,41,243,75,58,293,50,41,87,130,37,51,33,42,54,69,42,39,40,52,43,90,152,64,42,36,33,227

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,
Policy Iteration,
Q Learning,84,15,29,18,25,16,19,11,21,10,10,8,11,9,8,12,9,10,8,12,27,32,11,9,10,11,14,13,13,15,11,17,30,40,33,30,16,12,13,17,13,20,19,12,13,13,15,17,23,23,40,32,35,44,36,47,23,14,16,14,22,26,16,30,17,19,16,20,18,28

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-561.0,-51.0,-6.0,-120.0,-10.0,25.0,-908.0,-183.0,-256.0,-242.0,-35.0,24.0,28.0,10.0,-19.0,6.0,22.0,44.0,-19.0,38.0,65.0,-591.0,39.0,57.0,-3.0,-45.0,66.0,31.0,-10.0,12.0,42.0,49.0,43.0,-14.0,20.0,62.0,26.0,50.0,6.0,7.0,55.0,-28.0,71.0,61.0,-141.0,27.0,44.0,-191.0,52.0,61.0,15.0,-28.0,65.0,51.0,69.0,60.0,48.0,33.0,60.0,63.0,62.0,50.0,59.0,12.0,-50.0,38.0,60.0,66.0,69.0,-125.0

Running 90
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,662,135,134,262,141,261,145,84,239,37,60,57,90,132,93,41,152,212,84,46,113,362,69,152,119,65,51,53,91,199,45,79,57,83,132,39,105,71,58,164,80,43,93,120,49,208,77,80,43,95,68,139,52,48,46,60,79,61,119,37,41,45,99,59,44,54,69,87,91,64,53,51,343,104,60,39,53,94,38,66,87,71,94,45,45,89,37,524,184,29
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,<,v,>,<,v,^,*,*,^,>,<,*]
[*,*,*,*,*,>,v,v,<,>,^,*,*,v,v,>,<]
[*,*,*,*,*,>,>,^,v,^,<,v,v,v,v,<,<]
[*,*,*,*,*,*,^,>,v,>,<,<,<,<,v,v,<]
[*,*,*,*,*,*,^,<,>,<,*,^,<,^,<,^,v]
[*,*,*,*,*,*,<,^,^,<,*,*,<,^,<,<,<]
[*,*,*,*,*,^,v,v,v,v,*,*,*,^,^,<,>]
[*,*,*,*,*,^,<,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,^,v,<,v,>,<,*,*,*,*,*,*,^]
[*,*,*,*,v,<,v,>,^,>,*,*,*,*,*,>,^]
[^,*,*,^,>,>,^,^,v,*,*,*,*,*,*,>,^]
[<,<,<,^,^,>,v,>,*,*,*,*,*,*,*,^,^]
[>,v,v,^,v,v,v,>,*,*,*,*,*,*,*,^,>]
[v,>,>,^,<,>,v,*,*,*,*,*,*,*,>,>,^]
[>,v,^,v,^,<,>,*,*,*,*,*,*,*,^,^,^]
[^,v,<,<,>,^,>,*,*,*,*,*,*,*,>,^,^]
[<,v,v,v,v,^,<,*,*,*,*,*,*,*,v,^,^]
[>,>,<,v,<,^,^,*,*,*,*,*,*,*,>,^,^]
[>,v,>,v,^,v,v,*,*,*,*,*,*,*,v,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration,
Policy Iteration,
Q Learning,662,135,134,262,141,261,145,84,239,37,60,57,90,132,93,41,152,212,84,46,113,362,69,152,119,65,51,53,91,199,45,79,57,83,132,39,105,71,58,164,80,43,93,120,49,208,77,80,43,95,68,139,52,48,46,60,79,61,119,37,41,45,99,59,44,54,69,87,91,64,53,51,343,104,60,39,53,94,38,66,87,71,94,45,45,89,37,524,184,29

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration,
Policy Iteration,
Q Learning,92,17,18,32,26,10,6,7,9,15,8,7,11,9,11,15,8,14,15,24,27,11,11,11,11,14,10,18,11,14,31,32,35,39,13,13,13,14,12,13,12,15,13,15,15,20,13,40,30,44,39,41,42,33,23,23,18,12,22,18,14,17,16,26,20,15,19,22,16,15,20,24,35,27,63,45,54,52,47,37,20,20,26,24,19,27,24,28,32,18

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-560.0,-33.0,-32.0,-160.0,-39.0,-159.0,-43.0,18.0,-137.0,65.0,42.0,45.0,12.0,-30.0,9.0,61.0,-50.0,-110.0,18.0,56.0,-11.0,-260.0,33.0,-50.0,-17.0,37.0,51.0,49.0,11.0,-97.0,57.0,23.0,45.0,19.0,-30.0,63.0,-3.0,31.0,44.0,-62.0,22.0,59.0,9.0,-18.0,53.0,-106.0,25.0,22.0,59.0,7.0,34.0,-37.0,50.0,54.0,56.0,42.0,23.0,41.0,-17.0,65.0,61.0,57.0,3.0,43.0,58.0,48.0,33.0,15.0,11.0,38.0,49.0,51.0,-241.0,-2.0,42.0,63.0,49.0,8.0,64.0,36.0,15.0,31.0,8.0,57.0,57.0,13.0,65.0,-422.0,-82.0,73.0


//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////
///////////////////////                          /////////////////////////////
///////////////////////                          /////////////////////////////
///////////////////////         Bonus Run        /////////////////////////////
///////////////////////                          /////////////////////////////
///////////////////////                          /////////////////////////////
//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////

Running 150
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Q Learning Analysis//
Q Learning,227,342,107,198,288,141,400,79,200,123,198,150,78,93,40,147,125,81,33,48,91,49,92,237,289,47,47,74,136,43,320,60,533,54,90,217,72,28,121,53,96,135,33,61,42,36,61,75,67,69,71,71,76,199,39,45,47,108,50,34,39,46,38,36,67,52,59,79,1295,49,88,63,102,212,41,89,207,37,38,71,72,96,37,85,93,31,80,85,27,223,92,44,37,68,143,28,134,70,80,59,77,97,53,80,162,53,30,46,184,38,72,84,110,64,57,53,46,40,126,54,41,80,126,88,41,46,83,55,61,32,39,324,48,127,58,93,40,59,46,43,32,85,31,30,111,42,38,49,72,58
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,<,^,<,>,v,^,*,*,v,v,<,*]
[*,*,*,*,*,>,^,^,>,>,v,*,*,v,v,<,>]
[*,*,*,*,*,>,v,v,^,v,v,v,<,<,v,v,<]
[*,*,*,*,*,*,^,v,>,>,>,<,^,^,^,v,^]
[*,*,*,*,*,*,>,^,<,>,*,^,<,<,<,<,^]
[*,*,*,*,*,*,<,v,>,>,*,*,^,<,^,^,^]
[*,*,*,*,*,<,v,v,<,^,*,*,*,v,^,<,^]
[*,*,*,*,*,^,v,v,v,^,*,*,*,*,*,>,^]
[*,*,*,*,>,v,<,v,^,>,*,*,*,*,*,*,^]
[*,*,*,*,^,v,^,v,^,<,*,*,*,*,*,>,^]
[<,*,*,<,^,^,>,>,v,*,*,*,*,*,*,>,^]
[>,>,^,v,v,>,v,v,*,*,*,*,*,*,*,>,^]
[<,<,v,v,^,^,v,^,*,*,*,*,*,*,*,^,^]
[v,<,<,>,v,>,^,*,*,*,*,*,*,*,v,>,^]
[v,>,<,<,v,>,^,*,*,*,*,*,*,*,<,<,>]
[^,>,>,v,^,>,>,*,*,*,*,*,*,*,v,>,^]
[v,v,v,v,>,>,<,*,*,*,*,*,*,*,>,^,^]
[>,^,^,^,v,v,v,*,*,*,*,*,*,*,^,>,^]
[>,v,v,<,v,<,<,*,*,*,*,*,*,*,^,<,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150
Value Iteration,
Policy Iteration,
Q Learning,227,342,107,198,288,141,400,79,200,123,198,150,78,93,40,147,125,81,33,48,91,49,92,237,289,47,47,74,136,43,320,60,533,54,90,217,72,28,121,53,96,135,33,61,42,36,61,75,67,69,71,71,76,199,39,45,47,108,50,34,39,46,38,36,67,52,59,79,1295,49,88,63,102,212,41,89,207,37,38,71,72,96,37,85,93,31,80,85,27,223,92,44,37,68,143,28,134,70,80,59,77,97,53,80,162,53,30,46,184,38,72,84,110,64,57,53,46,40,126,54,41,80,126,88,41,46,83,55,61,32,39,324,48,127,58,93,40,59,46,43,32,85,31,30,111,42,38,49,72,58

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150
Value Iteration,
Policy Iteration,
Q Learning,76,23,20,18,26,13,17,9,17,9,7,8,13,10,8,19,9,9,17,21,27,12,9,18,21,11,11,14,13,21,33,29,36,23,13,14,13,13,13,14,15,17,14,16,15,18,14,26,40,43,59,41,41,34,16,24,16,19,18,18,18,22,17,23,19,15,19,16,21,15,19,27,19,28,61,64,51,49,57,36,22,19,19,28,28,18,20,21,25,23,19,30,24,21,21,21,31,27,23,26,26,25,27,25,26,69,58,61,117,57,80,47,29,24,25,29,26,26,34,23,23,33,27,33,24,29,33,32,27,23,26,34,32,27,30,35,32,33,31,38,31,31,28,31,31,47,35,34,25,29

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-125.0,-240.0,-5.0,-96.0,-186.0,-39.0,-298.0,23.0,-98.0,-21.0,-96.0,-48.0,24.0,9.0,62.0,-45.0,-23.0,21.0,69.0,54.0,11.0,53.0,10.0,-135.0,-187.0,55.0,55.0,28.0,-34.0,59.0,-218.0,42.0,-431.0,48.0,12.0,-115.0,30.0,74.0,-19.0,49.0,6.0,-33.0,69.0,41.0,60.0,66.0,41.0,27.0,35.0,33.0,31.0,31.0,26.0,-97.0,63.0,57.0,55.0,-6.0,52.0,68.0,63.0,56.0,64.0,66.0,35.0,50.0,43.0,23.0,-1193.0,53.0,14.0,39.0,0.0,-110.0,61.0,13.0,-105.0,65.0,64.0,31.0,30.0,6.0,65.0,17.0,9.0,71.0,22.0,17.0,75.0,-121.0,10.0,58.0,65.0,34.0,-41.0,74.0,-32.0,32.0,22.0,43.0,25.0,5.0,49.0,22.0,-60.0,49.0,72.0,56.0,-82.0,64.0,30.0,18.0,-8.0,38.0,45.0,49.0,56.0,62.0,-24.0,48.0,61.0,22.0,-24.0,14.0,61.0,56.0,19.0,47.0,41.0,70.0,63.0,-222.0,54.0,-25.0,44.0,9.0,62.0,43.0,56.0,59.0,70.0,17.0,71.0,72.0,-9.0,60.0,64.0,53.0,30.0,44.0
