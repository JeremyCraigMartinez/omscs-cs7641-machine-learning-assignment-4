Running 10
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,7624,3543,12848,52527,6167,334,220,69,117,66
Starting reachability analysis
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,v,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,v]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,^,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,^,<,^,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,<]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,>,^,>,^,>,^,*,*,*,*,*,>,^]
[<,*,*,>,^,^,>,^,^,*,*,*,*,*,*,>,^]
[>,>,>,>,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,>,^,>,^,^,*,*,*,*,*,*,*,^,^]
[>,^,^,>,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,>,^,>,^,*,*,*,*,*,*,*,^,>,>]
[>,^,>,>,^,^,^,*,*,*,*,*,*,*,>,v,^]
[v,v,v,^,>,^,^,*,*,*,*,*,*,*,>,>,>]
[>,>,v,>,>,^,^,*,*,*,*,*,*,*,>,^,^]
[>,>,v,<,^,>,^,*,*,*,*,*,*,*,>,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,7624,3543,12848,52527,6167,334,220,69,117,66
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration,
Policy Iteration,85,12,18,20,22,32,36,35,36,51
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10
Value Iteration Rewards,
Policy Iteration Rewards,-7522.0,-3441.0,-12746.0,-52425.0,-6065.0,-232.0,-118.0,33.0,-15.0,36.0
Q Learning Rewards,
Running 30
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,5580,4045,512,8153,107,387,44,119,33,84,52,35,28,23,24,30,33,24,27,29,35,28,23,33,24,27,31,31,30,29
Starting reachability analysis
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,<,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,
Policy Iteration,5580,4045,512,8153,107,387,44,119,33,84,52,35,28,23,24,30,33,24,27,29,35,28,23,33,24,27,31,31,30,29
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration,
Policy Iteration,16,15,14,16,51,63,84,86,41,47,49,49,52,61,65,67,77,74,77,75,76,75,139,236,237,142,101,96,100,106
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
Value Iteration Rewards,
Policy Iteration Rewards,-5478.0,-3943.0,-410.0,-8051.0,-5.0,-285.0,58.0,-17.0,69.0,18.0,50.0,67.0,74.0,79.0,78.0,72.0,69.0,78.0,75.0,73.0,67.0,74.0,79.0,69.0,78.0,75.0,71.0,71.0,72.0,73.0
Q Learning Rewards,

Running 50
/////Hard Grid World Analysis/////

This is your grid world:
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1]
[1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0]
[1,1,1,1,1,1,0,0,0,0,1,1,0,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0]
[1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,1,0]
[1,1,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0]
[0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,0]
[0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,3]
[5,0,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1]



//Policy Iteration Analysis//
Policy Iteration,1707,5556,6492,6951,2831,1109,85,132,77,81,38,31,35,26,24,40,29,22,30,26,23,35,43,28,41,32,27,37,31,21,33,31,42,24,30,30,27,30,27,33,26,28,26,30,35,23,30,23,24,32
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,>,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration,
Policy Iteration,1707,5556,6492,6951,2831,1109,85,132,77,81,38,31,35,26,24,40,29,22,30,26,23,35,43,28,41,32,27,37,31,21,33,31,42,24,30,30,27,30,27,33,26,28,26,30,35,23,30,23,24,32
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration,
Policy Iteration,92,20,17,45,23,50,81,49,71,56,55,71,70,123,179,148,72,84,97,98,94,84,103,226,172,93,97,99,114,125,121,132,222,323,258,127,137,133,137,137,147,144,150,160,162,164,160,163,174,186
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50
Value Iteration Rewards,
Policy Iteration Rewards,-1605.0,-5454.0,-6390.0,-6849.0,-2729.0,-1007.0,17.0,-30.0,25.0,21.0,64.0,71.0,67.0,76.0,78.0,62.0,73.0,80.0,72.0,76.0,79.0,67.0,59.0,74.0,61.0,70.0,75.0,65.0,71.0,81.0,69.0,71.0,60.0,78.0,72.0,72.0,75.0,72.0,75.0,69.0,76.0,74.0,76.0,72.0,67.0,79.0,72.0,79.0,78.0,70.0
Q Learning Rewards,

Running 70
Total policy iterations: 29
Policy Iteration,15576,27365,7543,4718,1194,1545,315,283,97,32,33,31,32,27,30,31,24,23,23,37,22,31,30,24,31,30,25,30,28,36,27,31,29,24,31,25,31,35,24,35,36,25,26,29,24,44,32,28,29,33,22,55,32,24,23,31,33,24,27,24,26,40,31,34,29,29,30,38,36,29
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,
Policy Iteration,15576,27365,7543,4718,1194,1545,315,283,97,32,33,31,32,27,30,31,24,23,23,37,22,31,30,24,31,30,25,30,28,36,27,31,29,24,31,25,31,35,24,35,36,25,26,29,24,44,32,28,29,33,22,55,32,24,23,31,33,24,27,24,26,40,31,34,29,29,30,38,36,29
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration,
Policy Iteration,98,39,29,34,45,40,99,111,132,73,77,73,78,74,72,82,90,91,127,194,378,177,155,125,109,112,98,112,107,115,268,297,142,109,110,124,139,132,115,121,127,131,131,135,134,136,144,142,144,154,163,161,160,167,173,188,237,232,239,239,246,250,251,254,259,262,275,264,276,289
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70
Value Iteration Rewards,
Policy Iteration Rewards,-15474.0,-27263.0,-7441.0,-4616.0,-1092.0,-1443.0,-213.0,-181.0,5.0,70.0,69.0,71.0,70.0,75.0,72.0,71.0,78.0,79.0,79.0,65.0,80.0,71.0,72.0,78.0,71.0,72.0,77.0,72.0,74.0,66.0,75.0,71.0,73.0,78.0,71.0,77.0,71.0,67.0,78.0,67.0,66.0,77.0,76.0,73.0,78.0,58.0,70.0,74.0,73.0,69.0,80.0,47.0,70.0,78.0,79.0,71.0,69.0,78.0,75.0,78.0,76.0,62.0,71.0,68.0,73.0,73.0,72.0,64.0,66.0,73.0
Q Learning Rewards,

Running 90
Total policy iterations: 67
Policy Iteration,1408,13960,5212,3209,5198,594,80,294,201,63,39,40,30,25,28,24,23,29,31,27,24,22,26,31,35,41,31,28,27,31,30,36,27,32,31,24,34,21,29,25,36,25,30,24,26,36,31,27,30,28,28,24,27,35,42,30,27,23,24,27,30,29,30,26,26,33,32,28,33,27,25,30,38,23,28,23,30,25,25,25,31,37,41,21,27,38,27,30,28,28
Starting reachability analysis
Finished reachability analysis; # states: 180
Passes: 23

This is your optimal policy:
num of rows in policy is 17
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,*]
[*,*,*,*,*,>,>,>,>,v,v,*,*,v,v,<,<]
[*,*,*,*,*,>,>,>,>,>,v,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,>,^,<,<,<,<,<,<]
[*,*,*,*,*,*,>,>,>,^,*,^,<,<,<,<,<]
[*,*,*,*,*,*,^,>,^,^,*,*,^,<,<,<,<]
[*,*,*,*,*,>,^,^,^,^,*,*,*,^,<,<,^]
[*,*,*,*,*,^,^,^,^,^,*,*,*,*,*,^,^]
[*,*,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^]
[*,*,*,*,^,^,^,^,^,^,*,*,*,*,*,>,^]
[v,*,*,>,^,^,^,^,^,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[>,>,>,^,^,^,^,^,*,*,*,*,*,*,*,^,^]
[^,>,>,^,^,^,^,*,*,*,*,*,*,*,>,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,^,^,^,^,^,^,*,*,*,*,*,*,*,^,^,^]
[^,>,>,>,^,^,^,*,*,*,*,*,*,*,^,^,*]



Num generated: 2776; num unique: 180
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration,
Policy Iteration,1408,13960,5212,3209,5198,594,80,294,201,63,39,40,30,25,28,24,23,29,31,27,24,22,26,31,35,41,31,28,27,31,30,36,27,32,31,24,34,21,29,25,36,25,30,24,26,36,31,27,30,28,28,24,27,35,42,30,27,23,24,27,30,29,30,26,26,33,32,28,33,27,25,30,38,23,28,23,30,25,25,25,31,37,41,21,27,38,27,30,28,28
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration,
Policy Iteration,93,23,34,43,28,74,42,41,36,55,54,55,61,143,148,90,67,70,76,80,75,88,97,240,178,85,96,136,131,132,117,124,129,278,285,122,120,125,157,162,131,143,141,147,162,163,167,176,178,181,169,185,176,198,185,210,201,187,207,218,205,215,216,209,228,237,236,243,238,252,254,247,261,266,258,264,261,267,263,271,306,300,344,313,313,311,342,321,305,335
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
Value Iteration Rewards,
Policy Iteration Rewards,-1306.0,-13858.0,-5110.0,-3107.0,-5096.0,-492.0,22.0,-192.0,-99.0,39.0,63.0,62.0,72.0,77.0,74.0,78.0,79.0,73.0,71.0,75.0,78.0,80.0,76.0,71.0,67.0,61.0,71.0,74.0,75.0,71.0,72.0,66.0,75.0,70.0,71.0,78.0,68.0,81.0,73.0,77.0,66.0,77.0,72.0,78.0,76.0,66.0,71.0,75.0,72.0,74.0,74.0,78.0,75.0,67.0,60.0,72.0,75.0,79.0,78.0,75.0,72.0,73.0,72.0,76.0,76.0,69.0,70.0,74.0,69.0,75.0,77.0,72.0,64.0,79.0,74.0,79.0,72.0,77.0,77.0,77.0,71.0,65.0,61.0,81.0,75.0,64.0,75.0,72.0,74.0,74.0
Q Learning Rewards,
