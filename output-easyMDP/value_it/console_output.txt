Num generated: 600; num unique: 50
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,3188,37056,3006,835,5822,3740,4592,2037,687,201,298,81,34,31,34,34,33,36,32,30,38,28,30,30,42,26,37,30,36,34,34,29,39,32,38,30,37,31,33,31
Policy Iteration,
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,42,2,2,2,2,3,4,4,4,5,4,5,6,6,6,6,7,8,9,8,15,22,23,21,22,22,23,23,25,26,26,28,30,13,14,14,14,14,15,15
Policy Iteration,
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration Rewards,-3086.0,-36954.0,-2904.0,-733.0,-5720.0,-3638.0,-4490.0,-1935.0,-585.0,-99.0,-196.0,21.0,68.0,71.0,68.0,68.0,69.0,66.0,70.0,72.0,64.0,74.0,72.0,72.0,60.0,76.0,65.0,72.0,66.0,68.0,68.0,73.0,63.0,70.0,64.0,72.0,65.0,71.0,69.0,71.0
Policy Iteration Rewards,
Q Learning Rewards,
