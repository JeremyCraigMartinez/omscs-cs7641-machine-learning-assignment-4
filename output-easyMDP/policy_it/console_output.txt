Num generated: 600; num unique: 50
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,
Policy Iteration,38489,894992,15136,6483,7460,150433,7355,31488,657,882,459,799,61773,188,38,34,32,37,33,34,35,29,29,36,34,31,29,30,32,29,37,29,28,27,32,38,29,28,34,26
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration,
Policy Iteration,51,7,4,4,5,6,7,8,8,9,9,11,11,11,11,12,12,14,15,90,18,18,19,18,17,19,20,21,23,26,29,28,104,26,26,26,27,28,28,31
Q Learning,

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40
Value Iteration Rewards,
Policy Iteration Rewards,-38387.0,-894890.0,-15034.0,-6381.0,-7358.0,-150331.0,-7253.0,-31386.0,-555.0,-780.0,-357.0,-697.0,-61671.0,-86.0,64.0,68.0,70.0,65.0,69.0,68.0,67.0,73.0,73.0,66.0,68.0,71.0,73.0,72.0,70.0,73.0,65.0,73.0,74.0,75.0,70.0,64.0,73.0,74.0,68.0,76.0
Q Learning Rewards,
